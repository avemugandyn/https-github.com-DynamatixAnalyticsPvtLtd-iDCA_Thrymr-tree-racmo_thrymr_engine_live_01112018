{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "from lxml import etree\n",
    "from time import time\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import cascaded_union\n",
    "import pdftableextract as pte\n",
    "import numpy as np\n",
    "import pdfquery\n",
    "import json\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdfquery\n",
    "\n",
    "import os\n",
    "import re\n",
    "from math import floor\n",
    "from pymongo import MongoClient\n",
    "class dbConf(object):\n",
    "    # connect to database\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    try:\n",
    "        mdb = client.racmo\n",
    "        Notifications = mdb.Notifications\n",
    "        PdfFiles=mdb.PdfFiles\n",
    "        Keywords=mdb.Keywords\n",
    "        Ticket=mdb.Ticket\n",
    "        Documents=mdb.Documents\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_DIR = \"/home/thrymr/Racmo/RacmoIT/process/Gestured documents 15-02-2018\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = [f for f in listdir(PDF_DIR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "se={f.split(\".\")[-1].lower() for f in fl}\n",
    "d=dict()\n",
    "for s in se:\n",
    "    d.update({s:0})\n",
    "\n",
    "# for f in fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDIR=\"/home/thrymr/Racmo/RacmoIT\"\n",
    "def df_to_json(df):\n",
    "    js = {}\n",
    "    vals = df[0].values\n",
    "    ls = []\n",
    "    for i, v in enumerate(vals):\n",
    "        if not v:\n",
    "            ls.append(ls[-1])\n",
    "        else:\n",
    "            ls.append(v)\n",
    "    df[0] = ls\n",
    "    for key, idx in df.groupby([0]).groups.items():\n",
    "        \n",
    "        js[key] = {}\n",
    "        \n",
    "        for val in df.loc[idx][[1,2]].values:\n",
    "            if val[0] == \"\" and val[1] == \"\":\n",
    "                js[key][\"_value\"] = \"\"\n",
    "            elif val[0] == \"\" and val[1] != \"\":\n",
    "                js[key][\"_value\"] = val[1]\n",
    "            elif val[0] != \"\" and val[1] == \"\":\n",
    "                js[key][\"_value\"] = val[0]\n",
    "            elif val[0] != \"\" and val[1] != \"\":\n",
    "                js[key][val[0]] = val[1]                \n",
    "        if(key==\"Documentos\"):\n",
    "            js[key] = df.loc[idx][[1,2]].values\n",
    "        if len(js[key]) == 1 and \"_value\" in js[key]:\n",
    "            js[key] = js[key][\"_value\"]\n",
    "    return js\n",
    "def dump_json():\n",
    "    _JSON = {}  \n",
    "    pdf_files = [f for f in listdir(PDF_DIR)\\\n",
    "                 if isfile(join(PDF_DIR,  f)) and \n",
    "                 f[-3:].lower() == \"pdf\" ]\n",
    "    \n",
    "    for pdf_file in pdf_files:#\n",
    "        pdf_path = join(PDF_DIR, pdf_file)\n",
    "        _JSON[pdf_file] = {\"filepath\" : pdf_path}\n",
    "        try:\n",
    "            pdf = pdfquery.PDFQuery(pdf_path)\n",
    "            pdf.load()\n",
    "            root = pdf.tree.getroot().getchildren()[0]\n",
    "            page_box = [float(x) for x in root.get(\"bbox\")[1:-1].split(\",\")]\n",
    "            tables, _ =\\\n",
    "            zip(\n",
    "                *sorted(\n",
    "                    [(p.bounds,p.area) for p in cascaded_union(\n",
    "                        [box(*[float(x) for x in node.get(\"bbox\")[1:-1].split(\",\")]) for node in root.iter() if node.tag == \"LTRect\"]\n",
    "                    )],\n",
    "                    key = lambda x : -x[1]\n",
    "                )\n",
    "            )\n",
    "            X = page_box[2]\n",
    "            Y = page_box[3]\n",
    "            xf = 11.69/X\n",
    "            yf = 8.27/Y\n",
    "            t1, t2 = tables\n",
    "            table_1_bbox = \":\".join(map(str,(t1[0]*xf - 0.1, (Y - t1[3])*yf - 0.1, t1[2]*xf + 0.1, (Y - t1[1])*yf + 0.1)))\n",
    "            table_2_bbox = \":\".join(map(str,(t2[0]*xf - 0.1, (Y - t2[3])*yf - 0.1, t2[2]*xf + 0.1, (Y - t2[1])*yf + 0.1)))\n",
    "\n",
    "            df1 =\\\n",
    "            pd.DataFrame(\n",
    "                pte.table_to_list(\n",
    "                    pte.process_page(\n",
    "                        pdf_path,\n",
    "                        \"1\",\n",
    "                        crop = table_1_bbox,\n",
    "                        pad=20\n",
    "                    ),\n",
    "                    \"1\"\n",
    "                )[1]\n",
    "            )\n",
    "\n",
    "            _JSON[pdf_file][\"table_1\"] = df_to_json(df1)\n",
    "            df2 = \\\n",
    "            pd.DataFrame(\n",
    "                pte.table_to_list(\n",
    "                    pte.process_page(\n",
    "                        pdf_path,\n",
    "                        \"1\",\n",
    "                        crop = table_2_bbox,\n",
    "                        pad=20\n",
    "                    ),\n",
    "                    \"1\"\n",
    "                )[1]\n",
    "            )\n",
    "            df2.columns = df2.iloc[0]\n",
    "            df2 = df2.reindex(df2.index.drop(0))\n",
    "            _JSON[pdf_file][\"table_2\"] = df2.to_json(orient='index')\n",
    "            mdb = dbConf.mdb\n",
    "            Ticket = dbConf.Ticket\n",
    "            ti = {\n",
    "                \"filename\": pdf_file,\n",
    "                \"JSON\": _JSON[pdf_file]\n",
    "            }\n",
    "            Tickets.insert_one(ti)\n",
    "        except Exception as e:\n",
    "            print(e, pdf_path)\n",
    "    \n",
    "    with open(join(UPDIR,\"all_tables_\"+str(time())+\".json\"), \"w\") as f:\n",
    "        f.write(json.dumps(_JSON))\n",
    "    return _JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'td' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3fbb9e211748>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtickets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'td' is not defined"
     ]
    }
   ],
   "source": [
    "tickets=[i['filename'] for i in td]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword(text):\n",
    "    ndb = dbConf.mdb\n",
    "    dd = ndb.get_collection('Keywords').find({})\n",
    "    occu=dict()\n",
    "    for j,it in enumerate(dd):\n",
    "        c=0\n",
    "        f=True\n",
    "        for k in it['keyword']:\n",
    "            if re.search(k, text, re.IGNORECASE):\n",
    "                c+=1\n",
    "            else:\n",
    "                f=False\n",
    "        if f and c>0:\n",
    "            if(it['file_class']!='TI'):\n",
    "                if it['file_class'] in occu:\n",
    "                    occu[it['file_class']]['count']+=1\n",
    "                    occu[it['file_class']]['keywords'].append(it['keyword'])\n",
    "                else:\n",
    "                    occu.update({it['file_class']:{'count':1,'keywords':[it['keyword']]}})\n",
    "    return occu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mdb = dbConf.mdb\n",
    "Ticket = dbConf.Ticket\n",
    "# mdb.get_collection('Tickets').find({})\n",
    "td=mdb.get_collection('Ticket').find({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.DataFrame(list(td))\n",
    "del df['_id']\n",
    "df['document']=df.apply(lambda x :{'document':x['JSON']['table_1']['Documentos']},axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text_2(path):\n",
    "    try:\n",
    "        pdf=pdfquery.PDFQuery(path)\n",
    "        pdf.load()\n",
    "        pdftext=\"\"\n",
    "        pgn=len(pdf.tree.getroot().getchildren())\n",
    "        for i in range(0,pgn):\n",
    "            root = pdf.tree.getroot().getchildren()[i]\n",
    "            for node in root.iter():\n",
    "                try:\n",
    "    #                 if node.tag == \"LTTextLineHorizontal\" or node.tag == \"LTTextBox\":\n",
    "                    if node.text:\n",
    "                        pdftext=pdftext+\" \"+node.text\n",
    "    #                     pdftext=pdftext+\"\\n\"\n",
    "                except Exception as e:\n",
    "                    print(node.tag, e)\n",
    "        return pdftext\n",
    "    except Exception as e:\n",
    "        return('Error:'+str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text(path):\n",
    "    pdf=pdfquery.PDFQuery(path)\n",
    "    pdf.load()\n",
    "    pdftext=\"\"\n",
    "    pgn=len(pdf.tree.getroot().getchildren())\n",
    "    for i in range(0,pgn):\n",
    "        root = pdf.tree.getroot().getchildren()[i]\n",
    "        for node in root.iter():\n",
    "            try:\n",
    "                if node.tag == \"LTTextLineHorizontal\" or node.tag == \"LTTextBox\":\n",
    "                    pdftext=pdftext+node.text\n",
    "                    pdftext=pdftext+\"\\n\"\n",
    "            except Exception as e:\n",
    "                print(node.tag, e)\n",
    "    return pdftext\n",
    "def get_rtf_text(path):\n",
    "    print('unrtf --text '+path)\n",
    "    text=os.popen('unrtf --text '+path).read()\n",
    "    print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "989"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickets=list(set(tickets))\n",
    "len(tickets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{80, 95, 98, 99, 100, 101, 103, 105, 107, 109, 111, 120, 121, 124, 125}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df[10:100]\n",
    "s={len(row['JSON']['table_1']['Documentos']) for ind,row in df.iterrows()}\n",
    "s        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_files=list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db': 0, 'json': 0, 'pdf': 499, 'png': 0, 'rtf': 508, 'zip': 0}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for f in principal_files:\n",
    "    d[ f.split(\".\")[-1].lower()]+=1\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def noti_to_excel(principal_files):\n",
    "    ls=list()\n",
    "    for(key,v) in principal_files.items():\n",
    "        print(key)\n",
    "        for pf in v:\n",
    "            text=\"\"\n",
    "            row={'Filename':pf,'Filetype':key}\n",
    "            if pf[-3:].lower()=='pdf':\n",
    "                text=get_pdf_text(join(PDF_DIR,pf))\n",
    "            if pf[-3:].lower()=='rtf':\n",
    "                text=os.popen('cd \"'+PDF_DIR+'\" && unrtf --text '+pf).read()\n",
    "            occu=get_keyword(text)\n",
    "            if occu:\n",
    "                tlist,klist,c=zip(*[(k,v['keywords'],v['count']) for k,v in occu.items()])\n",
    "                kl=\"\"\n",
    "                tl=\"\"\n",
    "                for tkeys in klist:\n",
    "                    for keys in tkeys:\n",
    "                        for k in keys:\n",
    "                            kl=kl+k\n",
    "                            kl=kl+\"+\"\n",
    "                        kl=kl[:-1]\n",
    "                        kl=kl+\",\"\n",
    "                        tl=tl+tlist[klist.index(tkeys)]+\",\"\n",
    "                kl=kl[:-1]\n",
    "                tl=tl[:-1]\n",
    "                row.update({'Keyword List':kl,'Keyword Type List':tl})\n",
    "                ks=set(tlist)\n",
    "                if(len(ks)==1):\n",
    "                    row.update({'Unique':'Yes'})\n",
    "                else:\n",
    "                    row.update({'Unique':'No'})\n",
    "       \n",
    "            else:\n",
    "                row.update({'Keyword List':'','Keyword Type List':'','Unique':'NA'})\n",
    "            ls.append(row)\n",
    "    exceldf=pd.DataFrame(ls)\n",
    "    exceldf.describe()\n",
    "    writer = pd.ExcelWriter('/home/thrymr/Result-Excel-'+str(time())+'.xlsx')\n",
    "    exceldf.to_excel(writer,'Sheet1')\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notification\n",
      "Ticket\n"
     ]
    }
   ],
   "source": [
    "docs={'Notification':list(pfiles),'Ticket':tickets}\n",
    "noti_to_excel(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len()\n",
    "oth=[len(i) for i in filegroup.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(oth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = [f for f in listdir(PDF_DIR)\\\n",
    "                 if isfile(join(PDF_DIR,  f)) ]\n",
    "ls=list()    \n",
    "for pdf_file in pdf_files:\n",
    "    ls.append(pdf_file.split('_'))\n",
    "df=pd.DataFrame(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/thrymr/file_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.cursor.Cursor at 0x7fb343d31310>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/home/thrymr/file_group.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets=df['min_filename'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(columns=[\"file\",\"group\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j = 0;\n",
    "for i, r in df.iterrows():\n",
    "    for f in ast.literal_eval(r.files):\n",
    "        tmp.loc[j] = [f, r.group]\n",
    "        j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5951, 2)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structured_files_dataframe(df):\n",
    "    tmp = pd.DataFrame(columns=[\"file\",\"group\"])\n",
    "    j = 0\n",
    "    for i, r in df.iterrows():\n",
    "        for f in ast.literal_eval(r.files):\n",
    "            tmp.loc[j] = [f, r.group]\n",
    "            j = j+1\n",
    "    assert tmp.apply(lambda x:x.file.split(\"_\")[3] == x.group, axis = 1).all()\n",
    "    tmp[\"ext\"] = tmp.apply(lambda x : x.file.split(\".\")[-1],axis =1)\n",
    "    tmp[\"length\"] = tmp.apply(lambda x : len(x.file),axis =1)\n",
    "    TI = []\n",
    "    OI = []\n",
    "    for case, indices in tmp.groupby(\"group\").groups.items():\n",
    "        ticket_index = tmp.loc[indices].length.values.argmin()\n",
    "        for i, idx in enumerate(indices):\n",
    "            if i == ticket_index:\n",
    "                TI.append(idx)\n",
    "            else:\n",
    "                OI.append(idx)\n",
    "\n",
    "    tmp['type'] = \"OTHER\"\n",
    "    tmp.loc[TI,\"type\"] = \"TICKET\"\n",
    "    return tmp\n",
    "\n",
    "df=pd.read_csv('/home/thrymr/file_group.csv')\n",
    "temp = get_structured_files_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ticket(pdf_path):\n",
    "        _JSON = {\"filepath\" : pdf_path}\n",
    "        try:\n",
    "            pdf = pdfquery.PDFQuery(pdf_path)\n",
    "            pdf.load()\n",
    "            root = pdf.tree.getroot().getchildren()[0]\n",
    "            page_box = [float(x) for x in root.get(\"bbox\")[1:-1].split(\",\")]\n",
    "            tables, _ =\\\n",
    "            zip(\n",
    "                *sorted(\n",
    "                    [(p.bounds,p.area) for p in cascaded_union(\n",
    "                        [box(*[float(x) for x in node.get(\"bbox\")[1:-1].split(\",\")]) for node in root.iter() if node.tag == \"LTRect\"]\n",
    "                    )],\n",
    "                    key = lambda x : -x[1]\n",
    "                )\n",
    "            )\n",
    "            X = page_box[2]\n",
    "            Y = page_box[3]\n",
    "            xf = 11.69/X\n",
    "            yf = 8.27/Y\n",
    "            t1, t2 = tables\n",
    "            table_1_bbox = \":\".join(map(str,(t1[0]*xf - 0.1, (Y - t1[3])*yf - 0.1, t1[2]*xf + 0.1, (Y - t1[1])*yf + 0.1)))\n",
    "            table_2_bbox = \":\".join(map(str,(t2[0]*xf - 0.1, (Y - t2[3])*yf - 0.1, t2[2]*xf + 0.1, (Y - t2[1])*yf + 0.1)))\n",
    "\n",
    "            df1 =\\\n",
    "            pd.DataFrame(\n",
    "                pte.table_to_list(\n",
    "                    pte.process_page(\n",
    "                        pdf_path,\n",
    "                        \"1\",\n",
    "                        crop = table_1_bbox,\n",
    "                        pad=20\n",
    "                    ),\n",
    "                    \"1\"\n",
    "                )[1]\n",
    "            )\n",
    "\n",
    "            _JSON[\"table_1\"] = df_to_json(df1)\n",
    "            df2 = \\\n",
    "            pd.DataFrame(\n",
    "                pte.table_to_list(\n",
    "                    pte.process_page(\n",
    "                        pdf_path,\n",
    "                        \"1\",\n",
    "                        crop = table_2_bbox,\n",
    "                        pad=20\n",
    "                    ),\n",
    "                    \"1\"\n",
    "                )[1]\n",
    "            )\n",
    "            df2.columns = df2.iloc[0]\n",
    "            df2 = df2.reindex(df2.index.drop(0))\n",
    "            _JSON[\"table_2\"] = df2.to_json(orient='index')\n",
    "            mdb = dbConf.mdb\n",
    "            Ticket = dbConf.Ticket\n",
    "            ti = {\n",
    "                \"filename\": pdf_file,\n",
    "                \"JSON\": _JSON[pdf_file]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return('Error:'+str(e))\n",
    "    \n",
    "        return str(_JSON)\n",
    "    \n",
    "\n",
    "def parse_other(pf):\n",
    "    text=\"\"\n",
    "    if pf[-3:].lower()=='pdf':\n",
    "                text=get_pdf_text_2(join(PDF_DIR,pf))\n",
    "    if pf[-3:].lower()=='rtf':\n",
    "                text=os.popen('cd \"'+PDF_DIR+'\" && unrtf --text '+pf).read()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "for i, r in temp[5947:].iterrows():\n",
    "    if r.type == \"TICKET\":\n",
    "        report.loc[i,\"response\"] = parse_ticket(join(PDF_DIR,r.file))\n",
    "    else:\n",
    "        report.loc[i,\"response\"] = parse_other(r.file)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_text(text):\n",
    "    try:\n",
    "        return text.encode('utf-8')\n",
    "    except Exception as e:\n",
    "        return text\n",
    "report[\"response\"] = report[\"response\"].apply(change_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf8' codec can't decode byte 0xf3 in position 370: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-303-001f98f6a5df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/thrymr/Report.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Sheet1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mto_excel\u001b[0;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes)\u001b[0m\n\u001b[1;32m   1543\u001b[0m         formatter.write(excel_writer, sheet_name=sheet_name, startrow=startrow,\n\u001b[1;32m   1544\u001b[0m                         \u001b[0mstartcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstartcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze_panes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreeze_panes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1545\u001b[0;31m                         engine=engine)\n\u001b[0m\u001b[1;32m   1546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1547\u001b[0m     def to_stata(self, fname, convert_dates=None, write_index=True,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/formats/excel.pyc\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine)\u001b[0m\n\u001b[1;32m    647\u001b[0m         writer.write_cells(formatted_cells, sheet_name,\n\u001b[1;32m    648\u001b[0m                            \u001b[0mstartrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstartrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstartcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m                            freeze_panes=freeze_panes)\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/excel.pyc\u001b[0m in \u001b[0;36mwrite_cells\u001b[0;34m(self, cells, sheet_name, startrow, startcol, freeze_panes)\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 \u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstartcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m             )\n\u001b[0;32m-> 1404\u001b[0;31m             \u001b[0mxcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_conv_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m             \u001b[0mstyle_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/openpyxl/cell/cell.pyc\u001b[0m in \u001b[0;36mvalue\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;34m\"\"\"Set the value and infer type and display options.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/openpyxl/cell/cell.pyc\u001b[0m in \u001b[0;36m_bind_value\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTRING_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE_STRING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/openpyxl/cell/cell.pyc\u001b[0m in \u001b[0;36mcheck_string\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# convert to unicode string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# string must never be longer than 32,767 characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf8' codec can't decode byte 0xf3 in position 370: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "writer = pd.ExcelWriter('/home/thrymr/Report.xlsx')\n",
    "report.to_excel(writer,'Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=report.fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=pd.read_csv('/home/thrymr/report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "final_report=report[['file','group','type']]\n",
    "for i,row in report.iterrows():\n",
    "    row=row.fillna('')\n",
    "    if row['file'] in prev_df['Filename'].values:\n",
    "        \n",
    "        final_report.loc[i,\"keyword list\"] = prev_df.loc[prev_df['Filename'] == row['file']][\"Keyword List\"].values[0]\n",
    "        final_report.loc[i,\"Keyword Type List\"] = prev_df.loc[prev_df['Filename'] == row['file']][\"Keyword Type List\"].values[0]\n",
    "        final_report.loc[i,\"type\"] = prev_df.loc[prev_df['Filename'] == row['file']][\"Filetype\"].values[0].upper()\n",
    "        final_report.loc[i,\"Unique\"] = prev_df.loc[prev_df['Filename'] == row['file']][\"Unique\"].values[0]\n",
    "        final_report.loc[i,\"Status\"]='Parsed'\n",
    "    elif row['response'][:5]=='Error':\n",
    "        final_report.loc[i,\"Status\"]='Cannot Parse'\n",
    "    else:\n",
    "        text=row['response']\n",
    "        occu=get_keyword(text)\n",
    "        r={}\n",
    "        if occu:\n",
    "                tlist,klist,c=zip(*[(k,v['keywords'],v['count']) for k,v in occu.items()])\n",
    "                kl=\"\"\n",
    "                tl=\"\"\n",
    "                for tkeys in klist:\n",
    "                    for keys in tkeys:\n",
    "                        for k in keys:\n",
    "                            kl=kl+k\n",
    "                            kl=kl+\"+\"\n",
    "                        kl=kl[:-1]\n",
    "                        kl=kl+\",\"\n",
    "                        tl=tl+tlist[klist.index(tkeys)]+\",\"\n",
    "                kl=kl[:-1]\n",
    "                tl=tl[:-1]\n",
    "                r.update({'Keyword List':kl,'Keyword Type List':tl})\n",
    "                ks=set(tlist)\n",
    "                if(len(ks)==1):\n",
    "                    r.update({'Unique':'Yes'})\n",
    "                else:\n",
    "                    r.update({'Unique':'No'})\n",
    "       \n",
    "        else:\n",
    "                r.update({'Keyword List':'','Keyword Type List':'','Unique':'NA'})\n",
    "        final_report.loc[i,\"keyword list\"] = r['Keyword List']\n",
    "        final_report.loc[i,\"Keyword Type List\"] =r['Keyword Type List']\n",
    "        final_report.loc[i,\"Unique\"] = r['Unique']\n",
    "        final_report.loc[i,\"Status\"]='Parsed'\n",
    "writer = pd.ExcelWriter('/home/thrymr/Final-Result-Excel-ALL-FILES.xlsx')\n",
    "final_report.to_excel(writer,'Sheet1')\n",
    "writer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
