{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdfquery\n",
    "import json\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import cascaded_union\n",
    "import pdftableextract as pte\n",
    "from math import floor\n",
    "import pickle\n",
    "from pymongo import MongoClient\n",
    "from os.path import isfile, join, isdir\n",
    "from os import listdir\n",
    "import re\n",
    "import hashlib\n",
    "import unidecode\n",
    "from fuzzysearch import find_near_matches\n",
    "import textract\n",
    "import unicodedata\n",
    "class dbConf(object):\n",
    "    # connect to database\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    try:\n",
    "        mdb = client.racmo\n",
    "        Notifications = mdb.Notifications\n",
    "        PdfFiles=mdb.PdfFiles\n",
    "        Keywords=mdb.Keywords\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "PDF_DIR = \"/home/thrymr/Racmo/RacmoIT/process/Gestured documents 15-02-2018\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../RacmoIT.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kdf=b['keywords']\n",
    "fdf=b['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileclass</th>\n",
       "      <th>filetype</th>\n",
       "      <th>purpose</th>\n",
       "      <th>notification_type</th>\n",
       "      <th>keyword</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [fileclass, filetype, purpose, notification_type, keyword, bias]\n",
       "Index: []"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Final_Keyword_Analysis.pickle', 'rb') as handle:\n",
    "    b=pickle.load(handle)\n",
    "kxddf=b['keywordsX']\n",
    "# fdf=fdf.rename(columns={'file':'filename','type':'filetype','group':'filegroup'})\n",
    "kxddf[kxddf['keyword'].apply(lambda x : 'EJECUCION DE TITULOS NO JUDICIALES'.lower() in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fgs=fdf.groupby('filegroup')\n",
    "# fgdf=pd.DataFrame(columns=['filegroup'])\n",
    "# i=0\n",
    "# fgdf['files']=np.empty((293, 0)).tolist()\n",
    "# fgdf['filetypes']=np.empty((293, 0)).tolist()\n",
    "# for k,v in fgs.groups.items():\n",
    "    \n",
    "#     fgdf.loc[i,'filegroup']=k\n",
    "#     files=[]\n",
    "#     pcs=[]\n",
    "#     filetypes=[]\n",
    "#     for ind in v:\n",
    "#             files.append(fdf.loc[ind,'filename'])\n",
    "#             filetypes.append(fdf.loc[ind,'filetype'])\n",
    "#     fgdf.loc[i,'filetypes']=filetypes\n",
    "#     fgdf.loc[i,'files']=files\n",
    "#     i+=1\n",
    "# fgdf          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for i ,r in fdf[~(fdf['table_response'].str.contains('Error:'))&(fdf['filetype']=='TICKET')].iterrows():\n",
    "    if r['table_response'][:5]!='Error':\n",
    "        js=json.loads(r['table_response'])\n",
    "        pf=r['filename'].split('.')[0]+'_'+''.join(js['table_1']['Documentos'][0][0].split()).split('(Principal)')[0]\n",
    "        fgf=[''.join(x.split())for x in fgdf.loc[fgdf['filegroup']==r['filegroup'],'files'].values[0] ]\n",
    "        if pf in fgf:\n",
    "            fl=fgdf.loc[fgdf['filegroup']==r['filegroup'],'files'].values[0][fgf.index(pf)]\n",
    "            fdf.loc[fdf['filename'].str.contains(fl),'filetype']=\"NOTIFICATION\"\n",
    "            c+=1\n",
    "print c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf=kxddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfclass={'N1':'ADMISSION','N2':'TRANSFER OF LEGAL REPRESENTATION PERMITTED',\n",
    "           'N3':'ENFORCEMENT ORDERS','N4':'STATEMENT OF PAYMENT',\n",
    "           'N5':'REJECTED CLAIMS','N6':'REJECTED TRANSFER OF LEGAL REPRESENTATION',\n",
    "           'N7':'HEARING','N8':'ASSET INQUIRY',\n",
    "           'N9':'PLACE OF RESIDENCY REQUIRED',\n",
    "           'N10':'REQUIREMENT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n"
     ]
    }
   ],
   "source": [
    "n=kdf.iterrows()\n",
    "for i,row in n:\n",
    "    if 'EJECUCION DE TITULOS NO JUDICIALES'.lower() in row['keyword'] or 'ejecucion de titulos judiciales' in row['keyword']:\n",
    "        kdf=kdf.drop(kdf.index[i])\n",
    "        kxddf=kdf.drop(kdf.index[i])\n",
    "        print i\n",
    "    elif(not ( row['fileclass'] =='N5'or row['fileclass'] =='N6')):\n",
    "        if(pdfclass[row['fileclass']] in row['notification_type']):\n",
    "            \n",
    "            kdf.loc[i,'purpose']='CLASSIFICATION'\n",
    "        else:\n",
    "            kdf.loc[i,'purpose']='EXTRACTION'\n",
    "    elif row['fileclass'] =='N5':\n",
    "        if('N5 - REJECTED CLAIMS' in row['notification_type']):\n",
    "            kdf.loc[i,'purpose']='CLASSIFICATION'\n",
    "        else:\n",
    "            kdf.loc[i,'purpose']='EXTRACTION'\n",
    "    elif row['fileclass'] =='N6':\n",
    "        if('N6 - REJECTED TRANSFER OF LEGAL REPRESENTATION' in row['notification_type']):\n",
    "            kdf.loc[i,'purpose']='CLASSIFICATION'\n",
    "        else:\n",
    "            kdf.loc[i,'purpose']='EXTRACTION'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r in fdf.iterrows():\n",
    "    try:\n",
    "        newtext =unicode(textract.process(join(PDF_DIR,r['filename'])),'utf-8')\n",
    "        newtext=(''.join((c for c in unicodedata.normalize('NFD', newtext) if unicodedata.category(c) != 'Mn'))).lower()\n",
    "        rem=''\n",
    "        paratlist=['MODO DE IMPUGNACION:'.lower(),'mode d\\'impugnacio','recurso de repelacion','recurs de reposicio','recurso de reposicion','recurso de apelacion']\n",
    "        if (r['filetype']=='NOTIFICATION') :\n",
    "            for parat in paratlist:\n",
    "                if (parat.lower() in newtext) :\n",
    "                    rem=re.split(r'\\s(?=(?:y firmo|y firma))',newtext.split(parat.lower())[-1],1)[0]\n",
    "            newtext=newtext.replace(rem,'')\n",
    "    except Exception as e:\n",
    "        newtext='Error:'+str(e)\n",
    "    fdf.loc[i,'text_response']=newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "kdf[\"document_list\"]=np.empty((len(kdf), 0)).tolist()\n",
    "fdf[\"keywords\"]=fdf[\"keywords\"].apply(lambda x:{})\n",
    "\n",
    "notextset=set()\n",
    "for j,row in fdf.iterrows():\n",
    "\n",
    "    if j%595==0:\n",
    "        print j\n",
    "    for i,kdrow in kdf.iterrows():\n",
    "        text=''.join(row['text_response'].split())\n",
    "#         \n",
    "        if row['text_response'][:5]!='Error':\n",
    "            \n",
    "            f=True\n",
    "            for k in kdrow['keyword']:\n",
    "#                 if not re.search(k, row['text_response'], re.IGNORECASE):\n",
    "                \n",
    "#                 print(unidecode.unidecode(k).lower(),find_near_matches(unidecode.unidecode(k).lower(), text, max_l_dist=1))\n",
    "#                 match=[]\n",
    "#                 try:\n",
    "#                     match=find_near_matches(str(unidecode.unidecode(k)).lower(), text, max_l_dist=2)\n",
    "#                 except Exception as e:\n",
    "#                     notextset.add(j)\n",
    "#                 if len(match)==0:\n",
    "#                     f=False\n",
    "                if not (''.join(unidecode.unidecode(k).split()).lower() in text):\n",
    "                    f=False\n",
    "            if f and (kdrow['filetype']==row['filetype'])and (kdrow['purpose']=='CLASSIFICATION'):\n",
    "                kdf.loc[i,\"document_list\"].append(row['hasid'])\n",
    "                \n",
    "                if kdrow['fileclass'] in fdf.loc[j,'keywords'].keys():\n",
    "                    fdf.loc[j,'keywords'][kdrow['fileclass']].append(kdrow['keyword'])\n",
    "                else:\n",
    "                    fdf.loc[j,'keywords'][kdrow['fileclass']]=list()\n",
    "                    fdf.loc[j,'keywords'][kdrow['fileclass']].append(kdrow['keyword'])\n",
    "# fdf\n",
    "# for idf,rowdf in fdf.iterrows():\n",
    "#     keywords=[]\n",
    "#     keywordclass=[]\n",
    "    \n",
    "#     for ki, kr in kddf.iterrows():\n",
    "#         if rowdf['hasid'] in kr['document_list'] :\n",
    "#             if kr['file_class'] in fdf.columns:\n",
    "#                 dokydf.loc[idf,kr['file_class']]+=1\n",
    "#             else:\n",
    "#                 dokydf[kr['file_class']]=0\n",
    "#                 dokydf.loc[idf,kr['file_class']]=1\n",
    "# .to_csv(\"keywords.csv\")  \n",
    "# keyrpo=pd.concat([keyrpo.drop(['key_count'], axis=1), keyrpo['key_count'].apply(pd.Series)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf['fileclass']=fdf['filename'].apply(lambda x : x.split()[0])\n",
    "# kdf.to_csv('testKey.csv')\n",
    "a = {'documents':fdf,'keywords':kdf}\n",
    "with open('Test.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dokydf=fdf.copy()[['filename','filetype','keywords']]\n",
    "keyrpodf=kdf.copy()\n",
    "for idf,rowdf in fdf.iterrows():\n",
    "    keywords=[]\n",
    "    keywordclass=[]\n",
    "    for k,v in rowdf['keywords'].items():\n",
    "        if 'key '+k in dokydf.columns:\n",
    "                dokydf.loc[idf,'key '+k].append(v)\n",
    "        else:\n",
    "                dokydf['key '+k]=np.empty((len(dokydf), 0)).tolist()\n",
    "                dokydf.loc[idf,'key '+k].append(v)\n",
    "    for ki, kr in kdf.iterrows():\n",
    "            if rowdf['hasid'] in kr['document_list'] :\n",
    "                if kr['fileclass'] in dokydf.columns:\n",
    "                    dokydf.loc[idf,kr['fileclass']]+=1\n",
    "                else:\n",
    "                    dokydf[kr['fileclass']]=0\n",
    "                    dokydf.loc[idf,kr['fileclass']]=1\n",
    "                if 'Filename '+ rowdf['fileclass'] in keyrpodf.columns:\n",
    "                    keyrpodf.loc[ki,'Filename '+ rowdf['fileclass']].append(rowdf['filename'])\n",
    "                else:\n",
    "                    keyrpodf['Filename '+ rowdf['fileclass']]=np.empty((len(keyrpodf), 0)).tolist()\n",
    "                    keyrpodf.loc[ki,'Filename '+ rowdf['fileclass']].append(rowdf['filename'])\n",
    "        \n",
    "\n",
    "# .to_csv(\"keywords.csv\")  \n",
    "# keyrpo=pd.concat([keyrpo.drop(['key_count'], axis=1), keyrpo['key_count'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'document_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-06a6f6b167d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrowdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mddf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkxddf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mrowdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hasid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fileclass'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mkr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/indexes/base.pyc\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   2572\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2573\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2574\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2575\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2576\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'document_list'"
     ]
    }
   ],
   "source": [
    "ddf=fdf.copy()[['filename','filegroup','hasid','fileclass','filetype']]\n",
    "for idf,rowdf in ddf.iterrows():\n",
    "    for ki, kr in kxddf.iterrows():\n",
    "        if rowdf['hasid'] in kr['document_list'] :\n",
    "            c=1\n",
    "            if not kr['fileclass']==kr['bias'] :\n",
    "#                 c=kr[kr['file_class']]\n",
    "                c=0\n",
    "            if kr['fileclass'] in ddf.columns:\n",
    "                ddf.loc[idf,kr['fileclass']]+=c\n",
    "            else:\n",
    "                ddf[kr['fileclass']]=0\n",
    "                ddf.loc[idf,kr['fileclass']]=c\n",
    "print(\"Part2\")\n",
    "\n",
    "for i,row in ddf.iterrows():\n",
    "    keyscore=dict(row[['N1','N2','N3','N4','N5','N6','N7','N8','N9','N10']])\n",
    "    predicted=max(keyscore, key=keyscore.get)\n",
    "    if row['N5']>0:\n",
    "        ddf.loc[i,'predicted_class']='N5'\n",
    "    elif row['N2']>0 and row['N4']>0:\n",
    "        ddf.loc[i,'predicted_class']='N2+N4'\n",
    "    elif row['N1']>0 and(row['N10']>0 or row['N9']>0):\n",
    "        ddf.loc[i,'predicted_class']='N1'\n",
    "    elif row['N10']>0 and (row['N2']>0 or row['N6']>0):\n",
    "        ddf.loc[i,'predicted_class']='N2+N4'\n",
    "    elif keyscore[predicted]>0:\n",
    "        ddf.loc[i,'predicted_class']=predicted\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgs=ddf.groupby('filegroup')\n",
    "i=0\n",
    "fgdf['predicted_classes']=np.empty((len(fgdf), 0)).tolist()\n",
    "for k,v in fgs.groups.items():\n",
    "    fgdf.loc[i,'filegroup']=k\n",
    "    pcs=[]\n",
    "    for ind in v:\n",
    "            pcs.append(ddf.loc[ind,'predicted_class'])\n",
    "    fgdf.loc[i,'predicted_classes']=pcs\n",
    "    i+=1\n",
    "fgdf          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_actufg=[]\n",
    "y_predfg=[]\n",
    "conflic=[]\n",
    "for k,v in fgs.groups.items():\n",
    "#     y_actufg.append(ddf.loc[v[0],'fileclass'])\n",
    "    s=set()\n",
    "    e='_alpha'\n",
    "    for i in v:\n",
    "        s.add(ddf.loc[i,'predicted_class'])\n",
    "#     s={x for x in s if x==x}\n",
    "    if(len(s)==1):\n",
    "        if list(s)[0]==list(s)[0]:\n",
    "            e=list(s)[0]\n",
    "            y_predfg.append(list(s)[0])\n",
    "    else:\n",
    "        \n",
    "        if(len({x for x in s if x==x})>1):\n",
    "            e='Conflict'\n",
    "             \n",
    "        else:\n",
    "            \n",
    "            for x in list(s):\n",
    "                if x==x:\n",
    "                    e=x\n",
    "    if e!='_alpha' and e!='Conflict':\n",
    "        fgdf.loc[(fgdf['filegroup'] == k),'group_predicted_class']=e\n",
    "    elif e=='Conflict':\n",
    "        for j in v:\n",
    "            if ddf.loc[j,'filetype']=='TICKET':\n",
    "                e=ddf.loc[j,'predicted_class']\n",
    "                fgdf.loc[(fgdf['filegroup'] == k),'group_predicted_class']=e\n",
    "#     y_predfg.append(e)        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'documents':fdf,'keywords':kdf,'filegroup':fgdf}\n",
    "with open('FinalTest.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('FinalTest.pickle', 'rb') as handle:\n",
    "    a=pickle.load(handle)\n",
    "fgdf=a['filegroup']\n",
    "fdf=a['documents']\n",
    "kdf=a['keywords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedures={'CAM':'Cambiario','COG':'Cognicion','CON':'Concurso Acreedores','EJE':'Juicio Ejecutivo','ETJ':'EJECUCION DE TITULOS JUDICIAL','ETJH':'ETJ Continuacion Hipotecario','ETN':'EJECUCION DE TITULOS NO JUDICIAL',\\\n",
    "            'HIP':'EJECUCION Hipotecaria','MCU':'Menor Cuantia','MON':'Monitorio','ORD':'‘Concurso Ordinario','TDO':'Terceria De Domino','TMD':'Terceria De Mejor Derecho','VER':'Verbal','PEN':'Penal','RAP':'Recurso De Apelacion',\\\n",
    "            'PTC':'Pieza Tasacion Costas','PSO':'Pieza Seperada Oposicion','AUX':'Auxilio Nacional','CNJ':'Cosignacion Judicial','RCS':'Recurso De Casacion','INC':'Incidente Concursal','MC':'Medidas Cautelares','CN':'Conciliacion'}\n",
    "numspan={\"1\":\"uno\",\"2\":\"dos\",\"3\":\"tres\",\"4\":\"cuatro\",\"5\":\"cinco\",\"6\":\"seis\",\"7\":\"siete\",\"8\":\"ocho\",\"9\":\"nueve\",\"10\":\"diez\",\"11\":\"once\",\"12\":\"doce\",\"13\":\"trece\",\"14\":\"catorce\",\"15\":\"quince\",\"16\":\"dieciseis\",\"17\" :\"diecisiete\",\"18\":\"dieciocho\",\"19\":\"diecinueve\",\"20\" : \"veinte\",\"21\" : \"veintiuno\",\"22\"  :\"veintidós\",\"23\" : \"veintitrés\",\"24\" : \"veinticuatro\",\"25\" : \"veinticinco\",\"26\":\"veintiséis\",\"27\" :\"veintisiete\",\"28\": \"veintiocho\",\"29\"  :\"veintinueve\",\"30\" : \"treinta\"}\n",
    "numcat={\"1\":\"un\",\"2\":\"dos\",\"3\":\"tres\",\"4\":\"quatre\",\"5\":\"cinc\",\"6\":\"sis\",\"7\":\"set\",\"8\":\"vuit\",\"9\":\"nou\",\"10\":\"deu\",\"11\":\"onze\",\"12\":\"dotze\",\"13\":\"tretze\",\"14\":\"catorze\",\"15\":\"quinze\",\"16\":\"setze\",\"17\" :\"disset\",\"18\":\"divuit\",\"19\":\"dinou\",\"20\" : \"vint\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "extKeywords=kdf[kdf['purpose']=='EXTRACTION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "extKeywords['notification_type']=extKeywords['notification_type'].apply(lambda x : x.split('-')[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "fgdf['Numlist']=[[] for _ in range(len(fgdf))]\n",
    "fgdf['MIN DAYS']=''\n",
    "c=0\n",
    "for fi,fr in fgdf.iterrows():\n",
    "    auto=\"\"\n",
    "    match=None\n",
    "    match1=set()\n",
    "    match2=set()\n",
    "    for i,r in fdf[(fdf['filetype']=='NOTIFICATION')&(fdf['filename'].isin(fr['files']))].iterrows():\n",
    "        s= r['text_response']\n",
    "        if len(s.strip())>0:\n",
    "            s=unidecode.unidecode('\\n'.join(list(filter(None,s.split('\\n')))).lower())\n",
    "\n",
    "            fgdf.loc[fi,'Court']=s.split('\\n')[0]\n",
    "            if 'procurador' in s.lower():\n",
    "                fgdf.loc[fi,'Solictor']=list(filter(None,s[s.lower().index('procurador')+10:].split('\\n')))[0]\n",
    "\n",
    "\n",
    "            ptype=\"\"\n",
    "\n",
    "            for x in procedures.values():\n",
    "\n",
    "                if x.lower() in ' '.join(s.split()):\n",
    "\n",
    "                    fgdf.loc[fi,'Procedure_Type']=x\n",
    "                    ptype=x\n",
    "                    break\n",
    "            if ptype!=\"\":\n",
    "                if fr['filegroup']=='INADMISION25':\n",
    "                            print ' YESSS'\n",
    "                if 'procedimiento' in s.lower():\n",
    "                    match=re.search(r'(\\d{1,20}/\\d{4})',s.split('procedimiento')[1])\n",
    "\n",
    "                else:\n",
    "                    s1=''.join(s.split())\n",
    "                    match1=set(re.findall(r'(\\d{1,20}/\\d{4})',s1))\n",
    "                    match2={'/'.join(x.split('/')[1:])for x in re.findall(r'(\\d{1,2}/\\d{1,2}/\\d{4})',s1)}\n",
    "\n",
    "            if not match is None :\n",
    "                    auto=match.group(0)\n",
    "            elif not (match1-match2) is None :\n",
    "                    auto=list(match1-match2)[0] if len(list(match1-match2))>0 else ''\n",
    "    if(fr['group_predicted_class']=='N1')or(fr['group_predicted_class']=='N3')or(fr['group_predicted_class']=='N4'):\n",
    "        for ki,kr in extKeywords[(extKeywords['notification_type'].str.contains('AMOUNT'))&(extKeywords['fileclass']==fr['group_predicted_class'])].iterrows():\n",
    "            for i,r in fdf[(fdf['filename'].isin(fr['files']))&(fdf['filetype']==kr['filetype'])].iterrows():\n",
    "                text=''.join(r['text_response'].split())\n",
    "    #         \n",
    "                if r['text_response'][:5]!='Error':\n",
    "                    \n",
    "                    f=True\n",
    "                    t=text\n",
    "                    for k in kr['keyword']:\n",
    "                        k1=''.join(unidecode.unidecode(k).split()).lower()\n",
    "                        if  k1 in t:\n",
    "                            t=t[t.index(k1)+len(k1):]\n",
    "                        else:\n",
    "                            f=False\n",
    "                    k1=' '.join(kr['keyword'][0].split())    \n",
    "                    if f :\n",
    "                        tex=' '.join(r['text_response'].split())\n",
    "\n",
    "                        fgdf.loc[fi,'Amount']=tex[tex.index(k1)+len(k1):].split()[0]\n",
    "    if((fr['group_predicted_class']=='N9')or(fr['group_predicted_class']=='N10')):\n",
    "#         for ki,kr in extKeywords[(extKeywords['notification_type'].str.contains('TIME FRAME'))&(extKeywords['fileclass']==fr['group_predicted_class'])].iterrows():\n",
    "            for i,r in fdf[(fdf['filename'].isin(fr['files']))&(fdf['filetype']=='NOTIFICATION')].iterrows():\n",
    "                text=' '.join(unidecode.unidecode(r['text_response']).split())\n",
    "    #           \n",
    "                nls=[]\n",
    "                nlc=[]\n",
    "                if r['text_response'][:5]!='Error':\n",
    "                    \n",
    "                    f=True\n",
    "                    t=text\n",
    "                    \n",
    "                    k1='dias'\n",
    "                    \n",
    "                    if  k1 in t.lower():\n",
    "                          \n",
    "                        nls+=  [t.lower().split()[i-1] for i, x in enumerate(t.lower().split()) if 'dias' in x ]\n",
    "                    elif  'dies' in t.lower():\n",
    "                          \n",
    "                        nlc+=  [t.lower().split()[i-1] for i, x in enumerate(t.lower().split()) if 'dies' in x ]\n",
    "                    else:\n",
    "                            f=False\n",
    "                    if f :\n",
    "                        min=100\n",
    "                        ls=[]\n",
    "                        if len(nls)>0:\n",
    "                            fgdf.at[fi,'Numlist']=nls\n",
    "                            ls=nls\n",
    "                            numbers=numspan\n",
    "                        elif len(nlc)>0:\n",
    "                            fgdf.at[fi,'Numlist']=nlc\n",
    "                            ls=nlc\n",
    "                            numbers=numcat\n",
    "                        for num in ls:\n",
    "                            if num in numbers.values():\n",
    "                                n=int([x for x in numbers.keys() if numbers[x]==num ][0])\n",
    "                                if n < min:\n",
    "                                    min=n\n",
    "                            elif num in numbers.keys():\n",
    "                                if int(num) < min:\n",
    "                                    min=int(num)\n",
    "                            else:\n",
    "                                print num\n",
    "                        if min!=100:\n",
    "                            fgdf.loc[fi,'Time Frame']=min\n",
    "                        \n",
    "    if (fr['group_predicted_class']=='N5')|(fr['group_predicted_class']=='N6')|(fr['group_predicted_class']=='N9'):\n",
    "        \n",
    "\n",
    "        for ki,kr in extKeywords[~(extKeywords['notification_type'].str.contains('TIME FRAME'))&(extKeywords['fileclass']==fr['group_predicted_class'])].iterrows():\n",
    "            for i,r in fdf[(fdf['filename'].isin(fr['files']))&(fdf['filetype']=='NOTIFICATION')].iterrows():\n",
    "                text=''.join(r['text_response'].split())\n",
    "    #         \n",
    "                if r['text_response'][:5]!='Error':\n",
    "                    \n",
    "                    f=True\n",
    "                    t=text\n",
    "                    for k in kr['keyword']:\n",
    "                        k1=''.join(unidecode.unidecode(k).split()).lower()\n",
    "                        if  k1 in t:\n",
    "                            t=t[t.index(k1)+len(k1):]\n",
    "                        else:\n",
    "                            f=False\n",
    "                    k1=' '.join(kr['keyword'][0].split())    \n",
    "                    if f :\n",
    "                        fgdf.loc[fi,fr['group_predicted_class']+'-Extraction']=[dict({kr['notification_type']:kr['keyword']})]\n",
    "    for i,r in fdf[(fdf['filetype']=='TICKET')&(fdf['filename'].isin(fr['files']))].iterrows():\n",
    "        \n",
    "            if r.table_response[:5]!='Error':\n",
    "                try:\n",
    "                    js= json.loads(unidecode.unidecode( json.dumps(json.loads(r.table_response), ensure_ascii=False)))\n",
    "                except Exception as e:\n",
    "                    js=json.loads(r.table_response)\n",
    "                table2=json.loads(js['table_2'])\n",
    "#                 print(r['filename'],js[\"table_1\"][[x for x in js[\"table_1\"].keys() if 'Fecha' in x][0]],table2['1'][[x for x in table2['1'].keys() if 'Fecha' in x][0]])\n",
    "                c+=1\n",
    "                fgdf.loc[fi,\"Document date\"]=js[\"table_1\"][[x for x in js[\"table_1\"].keys() if 'Fecha' in x][0]]\n",
    "                fgdf.loc[fi,\"Stamp date\"]=table2['1'][[x for x in table2['1'].keys() if 'Fecha' in x][0]]\n",
    "#                 print js\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    fgdf.loc[fi,'Solictor']=js[\"table_1\"]['Destinatarios']['_value']\n",
    "                    fgdf.loc[fi,'Court']=js[\"table_1\"]['Remitente'][[x for x in js[\"table_1\"]['Remitente'].keys() if str(x)[:6].lower()=='organo'][0]]\n",
    "\n",
    "                except Exception as e:\n",
    "                    c+=1\n",
    "                auto=\"\"\n",
    "                try:\n",
    "                    s=json.loads(unidecode.unidecode(r['table_response']))['table_1']['Datos del mensaje']['Procedimiento destino']\n",
    "\n",
    "                except Exception as e:\n",
    "                    s=str( json.loads(r['table_response'])['table_1']['Datos del mensaje'])\n",
    "        #             print str(e)\n",
    "                match1=set(re.findall(r'(\\d{1,20}/\\d{4})',s))\n",
    "                match2={'/'.join(x.split('/')[1:])for x in re.findall(r'(\\d{1,2}/\\d{1,2}/\\d{4})',s)}\n",
    "                \n",
    "                f=False\n",
    "                for x in procedures.keys():\n",
    "                    if x.lower() in  s.lower():\n",
    "                        f=True\n",
    "                        proced= procedures[x]\n",
    "                if f:\n",
    "                    fgdf.loc[fi,'Procedure_Type']=proced\n",
    "                if not (match1-match2) is None :\n",
    "                    auto=list(match1-match2)[0]\n",
    "                    \n",
    "\n",
    "    fgdf.loc[fi,'Auto']=auto\n",
    "            \n",
    "print c\n",
    "fgdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={k:0 for k in list(set(x for x in fgdf['group_predicted_class'].values.tolist() if x==x))}\n",
    "c=0\n",
    "for x in fgdf['group_predicted_class'].values.tolist():\n",
    "    if x==x:\n",
    "        d[x]+=1\n",
    "        c+=1\n",
    "print d ,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fgdf['filegroup'].values.tolist()\n",
    "fgdf['Keywords']=[[] for _ in range(len(fgdf))]\n",
    "for i , r in fgdf.iterrows():\n",
    "    for f in r['files']:\n",
    "        for k,v in fdf.loc[fdf['filename']==f]['keywords'].values[0].items():\n",
    "            \n",
    "            if len(fgdf.loc[i,'Keywords'])>0 and  k in fgdf.loc[i,'Keywords'][0].keys() :\n",
    "                fgdf.loc[i,'Keywords'][0][k]+=v\n",
    "            else:\n",
    "                fgdf.loc[i,'Keywords']=[{k:v}]\n",
    "fgdf['Keywords']=fgdf['Keywords'].apply(lambda x :x[0] if len(x)>0 else {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = \"asdfasfsfsdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d93387e492c4d6a45ea54014bfcdc0c78b71ce3c'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashlib.sha1((asdf+str(time.time())).encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionclass={'N1':'ADMISSION','N2':'TRANSFER OF LEGAL REPRESENTATION PERMITTED',\n",
    "           'N3':'ENFORCEMENT ORDERS','N4':'STATEMENT OF PAYMENT',\n",
    "           'N5':'REJECTED CLAIMS','N6':'REJECTED TRANSFER OF LEGAL REPRESENTATION',\n",
    "           'N7':'HEARING','N8':'ASSET INQUIRY',\n",
    "           'N9':'PLACE OF RESIDENCY REQUIRED',\n",
    "           'N10':'REQUIREMENT'}\n",
    "for i, r in fgdf.iterrows():\n",
    "        if r['group_predicted_class']==r['group_predicted_class'] and r['group_predicted_class']!='N2+N4':\n",
    "            fgdf.loc[i,'decision(Actual)']=r['group_predicted_class']+' '+decisionclass[r['group_predicted_class']]\n",
    "        elif r['group_predicted_class']=='N2+N4':\n",
    "            fgdf.loc[i,'decision(Actual)']='N2 '+decisionclass['N2']+'+'+'N4 '+decisionclass['N4']\n",
    "        else:\n",
    "            fgdf.loc[i,'decision(Actual)']=''\n",
    "        fgdf.loc[i,'Court(Actual)']=r['Court']\n",
    "        fgdf.loc[i,'Solictor(Actual)']=r['Solictor']\n",
    "        fgdf.loc[i,'Document date(Actual)']=r['Document date']\n",
    "        fgdf.loc[i,'Stamp date(Actual)']=r['Stamp date']\n",
    "        fgdf.loc[i,'Procedure_Type(Actual)']=r['Procedure_Type']\n",
    "        fgdf.loc[i,'Amount(Actual)']=r['Amount']\n",
    "        fgdf.loc[i,'Auto(Actual)']=r['Auto']\n",
    "        fgdf.loc[i,'Time Frame(Actual)']=r['MIN DAYS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'fg':fgdf}\n",
    "with open('/home/thrymr/Racmo/pro/TestFileGroup.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fgdf.loc[fgdf['filegroup']==fgr,'group_predicted_class(Actual)']\n",
    "fgdf.loc[fgdf['filegroup']==fgr,'Court(Actual)']\n",
    "fgdf.loc[fgdf['filegroup']==fgr,'Solictor(Actual)']\n",
    "fgdf.loc[fgdf['filegroup']==fgr,'Document date(Actual)']\n",
    "fgdf.loc[fgdf['filegroup']==fgr,'Stamp date(Actual)']\n",
    "fgdf.loc[fgdf['filegroup']==fgr,'Procedure_Type(Actual)']\n",
    "fgdf.loc[fgdf['filegroup']==fgr,'Amount(Actual)']\n",
    "fgdf.loc[fgdf['filegroup']==fgr,'Auto(Actual)']\n",
    "fgdf.loc[fgdf['filegroup']==fgr,'Time Frame(Actual)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_FOLDER = \"/home/thrymr/Racmo/pro\"\n",
    "\n",
    "with open(UPLOAD_FOLDER+'/TestFileGroup.pickle', 'rb') as handle:\n",
    "    a=pickle.load(handle)\n",
    "fgdf=a['fg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('/home/thrymr/RACMOIT_File_Group_Result.xlsx')\n",
    "fgdf.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
