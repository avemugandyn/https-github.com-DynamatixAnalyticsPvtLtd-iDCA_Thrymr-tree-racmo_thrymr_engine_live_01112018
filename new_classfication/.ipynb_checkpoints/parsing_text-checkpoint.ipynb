{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdfquery\n",
    "import json\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import cascaded_union\n",
    "import pdftableextract as pte\n",
    "from math import floor\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import hashlib\n",
    "import unidecode\n",
    "from fuzzysearch import find_near_matches\n",
    "import textract\n",
    "import unicodedata\n",
    "from pymongo import MongoClient\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "#from configuration.configuration import ConfigClass,DbConf\n",
    "import shutil\n",
    "import datetime\n",
    "import multiprocessing as mp\n",
    "import datefinder\n",
    "import zipfile\n",
    "import signal\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PDF_DIR = \"/home/racmo/upload_N17/\"\n",
    "temp_fdf = pd.DataFrame()\n",
    "classi_fdf = pd.DataFrame()\n",
    "extract_fdf = pd.DataFrame()\n",
    "\n",
    "#class Document_Analysis:\n",
    "    \n",
    "# SELECT max(batch_id) FROM file_classification;\n",
    "#     def keywordimport():\n",
    "#         engine = create_engine(ConfigClass.SQLALCHEMY_DATABASE_URI)\n",
    "        \n",
    "#         susp_df= pd.read_sql_query('SELECT k.id,k.file_class as fileclass,k.file_type as filetype,\\\n",
    "#                                    k.keyword,k.remove_class FROM suspend_keywords k',engine)\n",
    "#         keyword_df= pd.read_sql_query('SELECT k.id,k.file_class as fileclass,k.file_type as filetype,k.purpose,\\\n",
    "#                                     k.decision_type,k.keyword,k.bias as bias,k.sub as sub FROM keywords k',engine)\n",
    "#         keyword_df['sub']=keyword_df['sub'].apply(lambda x : json.loads(x) if x!=None else [])\n",
    "#         keyword_df['keyword']=keyword_df['keyword'].apply(lambda x : json.loads(x))\n",
    "\n",
    "#         return keyword_df, susp_df\n",
    "\n",
    "#Getting number of pages in a pdf file    \n",
    "#     def get_pgnum(filename):\n",
    "#         pdf = pdfquery.PDFQuery(ConfigClass.UPLOAD_FOLDER + \"/\" + filename)\n",
    "#         pdf.load()\n",
    "#         pgn = len(pdf.tree.getroot().getchildren())\n",
    "#         return pgn\n",
    "\n",
    " #Takes a dataframe of filegroups which also contains the list of files as its argument and returns a dataframe files with columns, name,file group and its filetype(right now its either ticket or other as the file with smallest name in a filegroup is ticket)\n",
    "def get_structured_files_dataframe(df):\n",
    "    tmp = pd.DataFrame(columns=[\"file\",\"group\"])\n",
    "    j = 0\n",
    "    for i, r in df.iterrows():\n",
    "        for f in r.files:\n",
    "            tmp.loc[j] = [f, r.group]\n",
    "            j = j+1\n",
    "    tmp[\"ext\"] = tmp.apply(lambda x : x.file.split(\".\")[-1],axis =1)\n",
    "    tmp[\"length\"] = tmp.apply(lambda x : len(x.file),axis =1)\n",
    "    TI, OI = [], []\n",
    "    for case, indices in list(tmp.groupby(\"group\").groups.items()):\n",
    "        ticket_index = tmp.loc[indices].length.values.argmin()\n",
    "        for i, idx in enumerate(indices):\n",
    "            if i == ticket_index:\n",
    "                TI.append(idx)\n",
    "            else:\n",
    "                OI.append(idx)\n",
    "    tmp['type'] = \"OTHER\"\n",
    "    tmp.loc[TI,\"type\"] = \"TICKET\"\n",
    "    return tmp\n",
    "\n",
    "    #takes a df and returns to json, the df is taken from the data extracted from the tables of pdfs using pdf tables extract\n",
    "def df_to_json(df):\n",
    "    js = {}\n",
    "    vals = df[0].values\n",
    "    ls = []\n",
    "    for i, v in enumerate(vals):\n",
    "        if not v:\n",
    "            ls.append(ls[-1])\n",
    "        else:\n",
    "            ls.append(v)\n",
    "    df[0] = ls\n",
    "    for key, idx in list(df.groupby([0]).groups.items()):\n",
    "        js[key] = {}\n",
    "        for val in df.loc[idx][[1,2]].values:\n",
    "            if val[0] == \"\" and val[1] == \"\":\n",
    "                js[key][\"_value\"] = \"\"\n",
    "            elif val[0] == \"\" and val[1] != \"\":\n",
    "                js[key][\"_value\"] = val[1]\n",
    "            elif val[0] != \"\" and val[1] == \"\":\n",
    "                js[key][\"_value\"] = val[0]\n",
    "            elif val[0] != \"\" and val[1] != \"\":\n",
    "                js[key][val[0]] = val[1]                \n",
    "        if(key==\"Documentos\"):\n",
    "            js[key] = df.loc[idx][[1,2]].values.tolist()\n",
    "        if len(js[key]) == 1 and \"_value\" in js[key]:\n",
    "            js[key] = js[key][\"_value\"]\n",
    "    return js\n",
    "\n",
    "    #takes the path of the pdf and returns its text using pdf query the text extracted will be sorted accoring to its y cooridinates of its bounding boxes\n",
    "def get_pdf_text(path):\n",
    "    try:\n",
    "        pdf=pdfquery.PDFQuery(path)\n",
    "        pdf.load()\n",
    "        pdftext=\"\"\n",
    "        pgn=len(pdf.tree.getroot().getchildren())\n",
    "        for i in range(0,pgn):\n",
    "            root = pdf.tree.getroot().getchildren()[i]\n",
    "            npg=[]\n",
    "            for node in root.iter():\n",
    "                try:\n",
    "        #             if node.tag == \"LTTextLineHorizontal\" or node.tag == \"LTTextBox\" or node.tag==\"LTTextBoxHorizontal\":\n",
    "                    if node.text and float(node.get(\"y1\"))>50 and float(node.get(\"x1\"))>50:\n",
    "                        npg.append(node)\n",
    "        #                     pdftext=pdftext+\"\\n\"\n",
    "                except Exception as e:\n",
    "                    print((node.tag, e))\n",
    "            npg=sorted(npg, key=lambda x: float(x.get(\"x0\")))\n",
    "            npg=sorted(npg, key=lambda x: round(float(x.get(\"y0\"))),reverse=True)\n",
    "            if len(npg)>0:\n",
    "                prev_y=round(float(npg[0].get(\"y0\")))\n",
    "                for x in npg: \n",
    "                    esc=\"\\n\"\n",
    "                    if round(float(x.get(\"y0\")))==prev_y:\n",
    "                        esc=\"|\"\n",
    "    #                         print(x.get(\"y0\"),x.get(\"y1\"),x.text)\n",
    "                    prev_y=round(float(x.get(\"y0\")))\n",
    "                    if len(x.text)> 0:\n",
    "                        pdftext+=esc+x.text\n",
    "        return pdftext\n",
    "    except Exception as e:\n",
    "        return('Error:'+str(e))\n",
    "\n",
    " #takes rtf files path and returns its text    \n",
    "def get_rtf_text(path):\n",
    "    text = os.popen('unrtf --text '+path).read()\n",
    "    return text\n",
    "\n",
    "    #takes the path of a pdf and extract table 1 and table 2 of tickets and returns its json \n",
    "def parse_ticket(pdf_path):\n",
    "    _JSON = {\"filepath\" : pdf_path}\n",
    "    try:\n",
    "        pdf = pdfquery.PDFQuery(pdf_path)\n",
    "        pdf.load()\n",
    "        root = pdf.tree.getroot().getchildren()[0]\n",
    "        page_box = [float(x) for x in root.get(\"bbox\")[1:-1].split(\",\")]\n",
    "        tables, _ =\\\n",
    "        list(zip(\n",
    "            *sorted(\n",
    "                [(p.bounds,p.area) for p in cascaded_union(\n",
    "                    [box(*[float(x) for x in node.get(\"bbox\")[1:-1].split(\",\")]) for node in root.iter() if node.tag == \"LTRect\"]\n",
    "                )],\n",
    "                key = lambda x : -x[1]\n",
    "            )\n",
    "        ))\n",
    "        X = page_box[2]\n",
    "        Y = page_box[3]\n",
    "        xf = 11.69/X\n",
    "        yf = 8.27/Y\n",
    "        t1, t2 = tables\n",
    "\n",
    "        table_1_bbox = \":\".join(map(str,(t1[0]*xf - 0.1, (Y - t1[3])*yf - 0.1, t1[2]*xf + 0.1, (Y - t1[1])*yf + 0.1)))\n",
    "        table_2_bbox = \":\".join(map(str,(t2[0]*xf - 0.1, (Y - t2[3])*yf - 0.1, t2[2]*xf + 0.1, (Y - t2[1])*yf + 0.1)))\n",
    "\n",
    "        df1 =\\\n",
    "        pd.DataFrame(\n",
    "            pte.table_to_list(\n",
    "                pte.process_page(\n",
    "                    pdf_path,\n",
    "                    \"1\",\n",
    "                    crop = table_1_bbox,\n",
    "                    pad=20\n",
    "                ),\n",
    "                \"1\"\n",
    "            )[1]\n",
    "        )\n",
    "        _JSON[\"table_1\"] = df_to_json(df1)\n",
    "        df2 = \\\n",
    "        pd.DataFrame(\n",
    "            pte.table_to_list(\n",
    "                pte.process_page(\n",
    "                    pdf_path,\n",
    "                    \"1\",\n",
    "                    crop = table_2_bbox,\n",
    "                    pad=20\n",
    "                ),\n",
    "                \"1\"\n",
    "            )[1]\n",
    "        )\n",
    "        df2.columns = df2.iloc[0]\n",
    "        df2 = df2.reindex(df2.index.drop(0))\n",
    "        _JSON[\"table_2\"] = df2.to_json(orient='index')\n",
    "    except Exception as e:\n",
    "            return('Error:'+str(e))\n",
    "    return json.dumps(_JSON, ensure_ascii=False)\n",
    "\n",
    "    #takes a path of a file(pdf and rtf) and extract its texts and remove its accents of spansish characters\n",
    "def parse_other(pf):\n",
    "    text=\"\"\n",
    "    try:\n",
    "        if pf[-3:].lower()=='rtf':\n",
    "            #print('rtf')\n",
    "            newtext = unidecode.unidecode(textract.process(pf).decode('utf-8'))\n",
    "            #newtext =textract.process(pf)\n",
    "            #newtext=str(newtext,'utf-8')\n",
    "            # newtext=newtext\n",
    "        else:\n",
    "            newtext= get_pdf_text(pf)\n",
    "        if(len(newtext.split())==0):\n",
    "            #print(\"scan\")\n",
    "            newtext =textract.process(pf,method='tesseract').decode('utf-8')\n",
    "        try:\n",
    "            newtext=(''.join((c for c in unicodedata.normalize('NFD', newtext) if unicodedata.category(c) != 'Mn'))).lower()\n",
    "        except Exception as e:\n",
    "            print((str(e)+\" binary \",pf[-3:].lower()))\n",
    "            newtext=(''.join((c for c in unicodedata.normalize('NFD', newtext.decode(\"utf-8\")) if unicodedata.category(c) != 'Mn'))).lower()\n",
    "        rem=''\n",
    "        paratlist=['MODO DE IMPUGNACION:'.lower(),'mode d\\'impugnacio',\n",
    "                   'recurso de repelacion','recurs de reposicio','recurso de reposicion','recurso de apelacion',\n",
    "                    'INTERPONER RECURSO DIRECTO DE REVISION']\n",
    "\n",
    "        for parat in paratlist:\n",
    "            if (parat.lower() in newtext) :\n",
    "                rem=newtext.split(parat.lower())[-1]\n",
    "        newtext=newtext.replace(rem,'')\n",
    "    except Exception as e:\n",
    "        newtext='Error:'+str(e)\n",
    "    return newtext\n",
    "\n",
    "def unzip_add(fdf,PDF_DIR):\n",
    "    ffdf = fdf.copy()\n",
    "    #import zipfile\n",
    "    try:\n",
    "        for i, r in fdf.iterrows():\n",
    "            zip_dict ={}\n",
    "            if r.filename[-3:].lower()=='zip':\n",
    "                z_files = []\n",
    "                #print((r.filename.split('.')[0]))\n",
    "                with zipfile.ZipFile(join(PDF_DIR,r.filename)) as z:\n",
    "                    for fileinzip in [x for x in z.namelist()]:\n",
    "                        if not os.path.isdir(fileinzip):\n",
    "                            try:\n",
    "                                #fileinzip = unidecode.unidecode(fileinzip)\n",
    "                                zfdir=join(PDF_DIR, os.path.basename(fileinzip))\n",
    "                            except Exception as e:\n",
    "                                fileinzip = unidecode.unidecode(fileinzip)\n",
    "                                zfdir=join(PDF_DIR, os.path.basename(fileinzip))\n",
    "                            try:\n",
    "                                with z.open(os.path.join(fileinzip)) as fz,open(zfdir, 'wb') as zfp:\n",
    "                                    shutil.copyfileobj(fz, zfp)\n",
    "                                    #os.remove(zfdir)\n",
    "                                    if fileinzip.count('.') > 1:\n",
    "                                        condition = '.' in fileinzip\n",
    "                                        while(condition):\n",
    "                                            ind = fileinzip.find('.')\n",
    "                                            if ind != fileinzip.rfind('.'):\n",
    "                                                fileinzip = fileinzip[0:ind] + '_' + fileinzip[ind+1:]\n",
    "                                            else:\n",
    "                                                condition = False\n",
    "\n",
    "                                    f_name = r.filename[:-4]+'_'+str(''.join(fileinzip.split()))\n",
    "                                    f_name = f_name.replace('/','_')\n",
    "                                    if r.filename[:-4] in f_name:\n",
    "                                        if not os.path.exists(join(PDF_DIR,f_name)) and not os.path.exists(join(PDF_DIR,unidecode.unidecode(f_name))):\n",
    "                                            shutil.move(zfdir, join(PDF_DIR,unidecode.unidecode(f_name)))\n",
    "                                            z_files.append({\"filename\":unidecode.unidecode(f_name),\"filegroup\":r['filegroup'],\\\n",
    "                                                \"filetype\":\"OTHER\",'ext':fileinzip[-3:],\"length\":len(fileinzip)})\n",
    "                                        else:\n",
    "                                            os.remove(zfdir)\n",
    "                            except Exception as e:\n",
    "                                 print(e)\n",
    "                zip_dict[r['filegroup']]= z_files                                                                  \n",
    "            if zip_dict !={}:\n",
    "                for k,val in list(zip_dict.items()):\n",
    "                    for v in val:\n",
    "                        ffdf = ffdf.append(pd.Series(v), ignore_index=True) \n",
    "    except Exception as e:\n",
    "        print(str(e))           \n",
    "    return ffdf         \n",
    "       \n",
    "\n",
    "#takes file data frame and returns its table response(table json) and text response\n",
    "def parsefile(file_name):\n",
    "    global PDF_DIR\n",
    "    global temp_fdf\n",
    "    def timeout(signum, frame):\n",
    "        raise ValueError('time out,a very specific bad thing happened.')\n",
    "    signal.signal(signal.SIGALRM, timeout)\n",
    "    df = temp_fdf.loc[temp_fdf['filename']==file_name]\n",
    "    zips_filegroup = {}\n",
    "    for i, r in df.iterrows():\n",
    "        ticresponse=\"\"\n",
    "        textresponse=\"\"\n",
    "        zip_dict = []\n",
    "        try:\n",
    "            signal.alarm(45)\n",
    "            while 1:\n",
    "                ticresponse= parse_ticket(join(PDF_DIR,r.filename))\n",
    "                if len(ticresponse)>5:\n",
    "                    signal.alarm(0)\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            ticresponse='Error:'+str(e)\n",
    "        try:\n",
    "            signal.alarm(45)\n",
    "            if r.filename[-3:].lower()!='zip':\n",
    "                while 1:\n",
    "                    if r.filetype=='TICKET':\n",
    "                        textresponse=ticresponse\n",
    "                    else:\n",
    "                        textresponse= parse_other(join(PDF_DIR,r.filename))\n",
    "                        if 'Error:' in textresponse:\n",
    "                            print((\"text\",i,textresponse))\n",
    "                    if len(ticresponse)>5:\n",
    "                        signal.alarm(0)\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            textresponse='Error:'+str(e)\n",
    "            if (r.filename[-3:].lower()=='rtf'):\n",
    "                print(\"___________________________________________________\")\n",
    "\n",
    "        df.loc[i,\"table_response\"] = ticresponse\n",
    "        df.loc[i,\"text_response\"] = textresponse\n",
    "    return df\n",
    "\n",
    "    # Once the table json is extracted we know which is the principal notification file and moves on to update it\n",
    "def update_filetype(fdf):\n",
    "    fgs=fdf.groupby('filegroup')\n",
    "    fgdf=pd.DataFrame(columns=['filegroup'])\n",
    "    i=0\n",
    "    fgdf['files']=np.empty((len(fgs.groups), 0)).tolist()\n",
    "    fgdf['filetypes']=np.empty((len(fgs.groups), 0)).tolist()\n",
    "    for k,v in list(fgs.groups.items()):\n",
    "        fgdf.loc[i,'filegroup']=k\n",
    "        files=[]\n",
    "        pcs=[]\n",
    "        filetypes=[]\n",
    "        for ind in v:\n",
    "                files.append(fdf.loc[ind,'filename'])\n",
    "                filetypes.append(fdf.loc[ind,'filetype'])\n",
    "        fgdf.loc[i,'filetypes']=filetypes\n",
    "        fgdf.loc[i,'files']=files\n",
    "        i+=1\n",
    "    for i ,r in fdf[~(fdf['table_response'].str.contains('Error:'))&(fdf['filetype']=='TICKET')].iterrows():\n",
    "        if r['table_response'][:5]!='Error':\n",
    "            js=json.loads(r['table_response'])\n",
    "            pf=r['filename'].split('.')[0]+'_'+''.join(js['table_1']['Documentos'][0][0].split()).split('(Principal)')[0]\n",
    "            fgf=[''.join(x.split())for x in fgdf.loc[fgdf['filegroup']==r['filegroup'],'files'].values[0] ]\n",
    "            if pf in fgf:\n",
    "                fl=fgdf.loc[fgdf['filegroup']==r['filegroup'],'files'].values[0][fgf.index(pf)]\n",
    "                if \"caratula\" in pf.lower():\n",
    "                    fdf.loc[fdf['filename'].str.contains(fl),'filetype']=\"CARATULA\"\n",
    "                else:\n",
    "                    fdf.loc[fdf['filename'].str.contains(fl),'filetype']=\"NOTIFICATION\"\n",
    "\n",
    "    return fdf,fgdf \n",
    "\n",
    "def read_pdf_n_insert(root_new):\n",
    "    global PDF_DIR\n",
    "    global temp_fdf\n",
    "    global classi_fdf\n",
    "    global extract_fdf\n",
    "    file_nm_error = list()\n",
    "\n",
    "    t_time = time.time()\n",
    "    PDF_DIR = root_new\n",
    "    pdf_files= [f for f in listdir(PDF_DIR) if isfile(join(PDF_DIR,  f))]\n",
    "    if len(pdf_files)>0:\n",
    "        ls=list()\n",
    "        for pdf_file in pdf_files:\n",
    "            if len(pdf_file.split('_'))>=4 and len(pdf_file.split('_')[2])<=4:\n",
    "                ls.append(pdf_file.split('_'))\n",
    "            else:\n",
    "                file_nm_error.append(pdf_file)\n",
    "\n",
    "        df=pd.DataFrame(ls)\n",
    "        df = df[pd.notnull(df[3])]\n",
    "        fg=list(df.groupby(3))\n",
    "        ls=[]\n",
    "        i=0\n",
    "        for k,gdf in  fg:\n",
    "            fglist=[]\n",
    "            elist=[]\n",
    "            flist=list()\n",
    "            for i, row in gdf.iterrows():\n",
    "                row=row.dropna()\n",
    "                fln='_'.join(list(row))\n",
    "                flist.append(fln)\n",
    "                elist.append(fln[-3:].lower())\n",
    "            fgroup={'group':k,'files':flist,'length':len(flist),'min_filename':min(flist, key=len),'extensions':elist}\n",
    "            ls.append(fgroup)\n",
    "        flgdf=pd.DataFrame(ls)\n",
    "        flgdf=flgdf.dropna(thresh=1,axis=1)\n",
    "        cmpt_fdf = get_structured_files_dataframe(flgdf)\n",
    "        cmpt_fdf= cmpt_fdf.rename(columns={\"file\":\"filename\",\"group\":\"filegroup\",\"type\":\"filetype\"})\n",
    "        cmpt_fdf = unzip_add(cmpt_fdf,PDF_DIR)\n",
    "        temp_fdf = cmpt_fdf\n",
    "        list_of_files = cmpt_fdf['filename'].values.tolist()\n",
    "        print(\"Parsing start\")\n",
    "        parsing_start_t = time.time()\n",
    "        pool = mp.Pool(processes=mp.cpu_count())\n",
    "        results = pool.map_async(parsefile,list_of_files)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        print(\"Parsing complete(time in minutes) = \",(float(time.time()-parsing_start_t)/60))\n",
    "        output = results.get()\n",
    "        final_fdf=pd.concat(output)\n",
    "        fdf,fgdf=update_filetype(final_fdf)\n",
    "    return fdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/173948173_Diligencia de ordenacion texto libre.RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/173003794_Diligencia de ordenacion texto libre.RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/174324790_D.O queden autos a disposicion del Juez para resolver.RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/173382763_Prov. texto libre.RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/815.3 .RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/174092506_Diligencia de ordenacion texto libre.RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/174319567_Diligencia de ordenacion quedando actuaciones mesa SS┬¬. art. 540.RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/21664993_TESTIMONIO.PDF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/21664994_PODER.PDF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/21664995_PERSONACION ES.PDF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/174536607_D.O queden autos a disposicion del Juez para resolver.RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/174548818_Decreto admision  a tramite monitorio Dar cuenta  para examinar clausulas art. 815.4 .RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/174153923_D.O queden autos a disposicion del Juez para resolver.RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/174547394_DIOR. Dar cuenta a su SS┬¬ resolver art 815.1 815.3 .RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/174395356_Diligencia de ordenacion texto libre.RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/174739714_Decreto admision  a tramite monitorio Dar cuenta  para examinar clausulas art. 815.4 .RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/174203287_D.O queden autos a disposicion del Juez para resolver.RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/174586542_DIOR. Dar cuenta a su SS┬¬ resolver art 815.1 815.3 .RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/174808115_Dior. Por presentado informe semestral cumplimiento convenio y traslado.RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/21539650_TRASLADO PROCURADORES.PDF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/21539651_CUMPLIMIENTO CONVENIO.PDF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/21539652_ESCRITO CUMPLIMIENTO CONVENIO.PDF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/174817623_Diligencia de ordenacion texto libre.RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/21965681_CONSULTA FALLECIMIENTO DEUDOR.PDF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/174130625_Diligencia de ordenacion texto libre.RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N17/174130626_Prov. texto libre.RTF'\n",
      "Parsing start\n",
      "('text', 163, 'Error:time out,a very specific bad thing happened.')\n",
      "('text', 248, 'Error:time out,a very specific bad thing happened.')\n",
      "('text', 300, 'Error:time out,a very specific bad thing happened.')\n",
      "Parsing complete(time in minutes) =  10.779105889797211\n"
     ]
    }
   ],
   "source": [
    "fdf = read_pdf_n_insert(\"/home/racmo/upload_N17/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.to_excel(\"/home/thrymr/552-python-Workspace/shashank_classfication/upload_N17_parse_22_10.xlsx\")\n",
    "#dc.read_pdf_n_insert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OTHER', 'TICKET'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf.filetype.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "newtext =textract.process('/home/racmo/upload_N17/2015_0005652_FIC_20181023080359020181001083011_030_16024_20180928_1000_0017367318_02.rtf')\n",
    "newtext = ' '.join(newtext.split()).replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = pd.read_excel(\"/home/thrymr/552-python-Workspace/shashank_classfication/upload_N17_parse.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N17_key = [unidecode.unidecode(x).lower() for x in ['DESE CUENTA A S.Sa','SE DA CUENTA A S.Sa','DAR CUENTA A S.Sa','ACUERDO DAR CUENTA A S.Sa','DESE CUENTA A LA MAGISTRADO JUEZ']]\n",
    "N17_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"2015_0005652_FIC_20181023080359020181001083011_030_16024_20180928_1000_0017367318_02.rtf\"\n",
    "for i,r in fdf.loc[fdf['filename']==f].iterrows():\n",
    "    text = r['text_response'] \n",
    "    print(text)\n",
    "    for k in N17_key:\n",
    "        if k in text:\n",
    "            print(k,\"--- Present in Text ---\")\n",
    "        match=find_near_matches(unidecode.unidecode(k).lower(), text, max_l_dist=1, max_insertions=2)\n",
    "        \n",
    "        print('\\\\n',match)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
