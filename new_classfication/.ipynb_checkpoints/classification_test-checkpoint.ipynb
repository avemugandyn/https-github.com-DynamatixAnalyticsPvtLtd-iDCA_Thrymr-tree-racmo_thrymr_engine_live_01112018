{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdftableextract as pte\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdfquery\n",
    "import json\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import cascaded_union\n",
    "import shutil\n",
    "from math import floor\n",
    "import pickle\n",
    "from pymongo import MongoClient\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "import time\n",
    "from os import listdir\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import hashlib\n",
    "import unidecode\n",
    "from fuzzysearch import find_near_matches\n",
    "import textract\n",
    "import unicodedata\n",
    "import ast\n",
    "import zipfile\n",
    "import warnings\n",
    "import datefinder\n",
    "from ast import literal_eval\n",
    "\n",
    "PDF_DIR = \"/home/thrymr/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Keyword.pickle', 'rb') as handle:\n",
    "    b=pickle.load(handle, encoding='latin1')\n",
    "    \n",
    "kdf=b['keywords']\n",
    "suspkdf=b['susKeyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2682, 8), (333, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdf.shape, suspkdf.shape,#kdf.loc[kdf['fileclass']=='N35']\n",
    "#list(kdf.loc[(kdf.fileclass == 'N18')]['keyword'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_file = pd.ExcelFile(\"New_Classification_Expected_Results_v.02.xlsx\")\n",
    "\n",
    "correct_class_dfs = {sheet_name: xl_file.parse(sheet_name) \n",
    "          for sheet_name in xl_file.sheet_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "notification_corelation_dict = { 'N1' : {'N1','N4','N7','N9', 'N11','N13'},\n",
    "                                 'N2' : {'N2','N4','N7','N8','N11','N13','N15','N16','N36'},\n",
    "                                 'N3' : {'N3','N4','N7','N8','N11','N13','N15','N16','N19','N36'},\n",
    "                                 'N4' : {'N1','N2','N3','N4','N7','N8','N9','N10','N13','N14','N15','N16','N19','N24'},\n",
    "                                 'N5' : {'N5'},\n",
    "                                 'N6' : {'N6','N36'},\n",
    "                                 'N7' : {'N1','N2','N3','N4','N7','N11'},\n",
    "                                 'N8' : {'N2','N3','N4','N8','N9','N10','N11','N18','N19','N36'},\n",
    "                                 'N9' : {'N4','N8','N9','N10','N11','N13','N15','N16','N17','N35','N36'},\n",
    "                                 'N10' : {'N4','N8','N9','N10','N11','N12','N13','N15','N16','N17','N18','N19','N35','N36'},\n",
    "                                 'N11' : {'N1','N2','N3','N7','N8','N9','N10','N11','N13','N14','N15','N16','N18','N19','N36'},\n",
    "                                 'N12' : {'N10','N12'},\n",
    "                                 'N13' : {'N1','N2','N3','N4','N9','N10','N11','N13','N17','N35','N36'},\n",
    "                                 'N14' : {'N4','N11','N14'},\n",
    "                                 'N15' : {'N2','N3','N4','N9','N10','N11','N15','N16','N18','N19','N36'},\n",
    "                                 'N16' : {'N2','N3','N4','N9','N10','N11','N15','N16','N18','N19','N36'},\n",
    "                                 'N17':{'N9','N10','N13','N17','N35','N36'},\n",
    "                                 'N18':{'N8','N10','N11','N15','N16','N18','N19','N36'},\n",
    "                                 'N19':{'N3','N4','N8','N10','N11','N15','N16','N18','N19','N36'},\n",
    "                                 'N24':{'N4','N5','N6','N11','N14','N20','N21','N22','N24','N33','N34'},\n",
    "                                 'N34':{'N14','N24','N34'},\n",
    "                                 'N35':{'N9','N10','N13','N17','N20','N27','N35','N45'},\n",
    "                                 'N36':{'N2','N3','N6','N8','N9','N10','N11','N13','N15','N16','N17','N18','N19','N20','N27','N36','N45'}\n",
    "                               } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predclass_normal(kdf,row,j,text,fdf):\n",
    "    bias=row['bias']\n",
    "    if len(row['sub'])>0:\n",
    "        for i, kr in kdf[(kdf['keywordHash'].isin(row['sub']))].iterrows():\n",
    "            f = True\n",
    "            for k in kr['keyword']:\n",
    "                kw = unidecode.unidecode(k)\n",
    "                if not (re.search(r'\\b' + kw + r'\\b', text)):\n",
    "                     f=False\n",
    "                if not f:\n",
    "                    if (''.join(kw.split()).lower() in ''.join(text.split()).lower()):\n",
    "                        f=True\n",
    "                if f:\n",
    "                    bias1=get_predclass_normal(kdf,kr,j,text,fdf)\n",
    "                    bias=bias1\n",
    "                if kr['fileclass'] in list(fdf.loc[j,'keywords'].keys()):\n",
    "                    fdf.loc[j,'keywords'][kr['fileclass']].append(kr['keyword'])\n",
    "                else:\n",
    "                    fdf.loc[j,'keywords'][kr['fileclass']]=list()\n",
    "                    fdf.loc[j,'keywords'][kr['fileclass']].append(kr['keyword'])\n",
    "    return bias\n",
    "\n",
    "def get_predclass_fuzzy(kdf,row,j,text,fdf):\n",
    "    bias = row['bias']\n",
    "    if len(row['sub'])>0:\n",
    "        for i, kr in kdf[(kdf['keywordHash'].isin(row['sub']))].iterrows():\n",
    "            f=True\n",
    "            for k in kr['keyword']:\n",
    "                try:\n",
    "                    match=find_near_matches(unidecode.unidecode(k).lower(), text, max_l_dist=1, max_insertions=2)\n",
    "                except Exception as e:\n",
    "                    f=False\n",
    "                if len(match)==0:\n",
    "                    f=False\n",
    "            if f:\n",
    "                bias1= get_predclass_fuzzy(kdf,kr,j,text,fdf)\n",
    "                bias=bias1\n",
    "                if kr['fileclass'] in list(fdf.loc[j,'keywords'].keys()):\n",
    "                    fdf.loc[j,'keywords'][kr['fileclass']].append(kr['keyword'])\n",
    "                else:\n",
    "                    fdf.loc[j,'keywords'][kr['fileclass']]=list()\n",
    "                    fdf.loc[j,'keywords'][kr['fileclass']].append(kr['keyword'])\n",
    "    return bias\n",
    "\n",
    "def get_classify_result(fdf,kdf,suspkdf,notification_corelation_dict):\n",
    "    kdf[\"document_list\"]=np.empty((len(kdf), 0)).tolist()\n",
    "    fdf[\"keywords\"]=fdf[\"filename\"].apply(lambda x:{})\n",
    "    fdf[\"pred_class\"]=fdf[\"filename\"].apply(lambda x:list())\n",
    "    fdf[\"remove_class\"]=fdf[\"filename\"].apply(lambda x:list())\n",
    "    fdf[\"after_classfn\"]=fdf[\"filename\"].apply(lambda x:list())\n",
    "    fdf[\"final_categ\"]=fdf[\"filename\"].apply(lambda x:list())\n",
    "    fdf.loc[fdf['filetype']=='TICKET','text_response']=fdf['table_response']\n",
    "    NX_filename_N1_N5=['S05','S02','CNO','S5L','S05','S04','S01','CNA','S5C','PCO','ASE','S3A','ICO','S1C']\n",
    "    classlis=[]\n",
    "    caratula_keywords = ['NOTIFICACION TELEMATICA', 'DENTRO DEL ARCHIVO COMPRIMIDO']\n",
    "\n",
    "    n2_prec_keywords = ['PARTE DISPOSITIVA', 'ANTECEDENTES DE HECHO', 'DILIGENCIA DE ORDENACIÓN']\n",
    "\n",
    "    for j, row in fdf.iterrows():\n",
    "        if row['filetype']!='TICKET':\n",
    "            if isinstance(row.text_response, (str,)):\n",
    "                text = (row['text_response']).lower()\n",
    "                text = unidecode.unidecode(text)\n",
    "                text = re.sub(' +',' ', text)\n",
    "                text = text.replace('\\n', '')\n",
    "                text = text.replace('\\xaa', '')\n",
    "                text = ' '.join(text.split())\n",
    "                text = text.replace('|','')\n",
    "                paratlist=['MODO DE IMPUGNACION:', 'mode d\\'impugnacio', 'recurso de repelacion',\n",
    "                           'recurs de reposicio', 'recurso de reposicion', 'recurso de apelacion', \n",
    "                           'INTERPONER RECURSO DIRECTO DE REVISION']\n",
    "                rem=''\n",
    "                if row['filename'] == \"2014_0000018_ENJ_20181023300913920181010190144_084_8867435_2018_I_174961916.PDF\":\n",
    "                    print(text)\n",
    "                for parat in paratlist:\n",
    "\n",
    "                    rem_word = parat.lower()\n",
    "                    if (rem_word in text):\n",
    "                        rem=text.split(rem_word)[-1]\n",
    "                    text=text.replace(rem,'')\n",
    "\n",
    "                if all(word.lower() in text for word in caratula_keywords):\n",
    "                    fdf.loc[j, 'filetype'] = 'CARATULA'\n",
    "\n",
    "                for i,kdrow in kdf.iterrows():\n",
    "                    if text[:5]!='Error':\n",
    "                        f=True\n",
    "                        for k in kdrow['keyword']:\n",
    "                            kw = unidecode.unidecode(k)\n",
    "                            if kw == 'entregar':\n",
    "                                search_for_keyword = 'entrega'\n",
    "                                if not (re.search(r'\\b' + search_for_keyword + r'\\b', text)):\n",
    "                                    f = False\n",
    "                            elif not (re.search(r'\\b' + kw + r'\\b', text)):\n",
    "                                f=False\n",
    "\n",
    "                        if f and (kdrow['filetype']==row['filetype'] or(kdrow['filetype']=='NOTIFICATION' and row['filetype']=='OTHER') )and (kdrow['purpose']=='CLASSIFICATION'):\n",
    "                            fdf.loc[j,'pred_class'].append(get_predclass_normal(kdf,kdrow,j,text,fdf))\n",
    "                            if kdrow['fileclass'] in list(fdf.loc[j,'keywords'].keys()):\n",
    "                                fdf.loc[j,'keywords'][kdrow['fileclass']].append(kdrow['keyword'])\n",
    "                            else:\n",
    "                                fdf.loc[j,'keywords'][kdrow['fileclass']]=list()\n",
    "                                fdf.loc[j,'keywords'][kdrow['fileclass']].append(kdrow['keyword'])\n",
    "                if row['filename'] == \"2014_0000018_ENJ_20181023300913920181010190144_084_8867435_2018_I_174961916.PDF\":\n",
    "                    print(\"bool(fdf.loc[j,'keywords'])\",bool(fdf.loc[j,'keywords']))\n",
    "                if True or not bool(fdf.loc[j,'keywords']):\n",
    "                    special_keywords = ['facilite','despachar','admitir','oigase', 'averiguar','requerir','requeriu']\n",
    "                    for i,kdrow in kdf.iterrows():\n",
    "                        if text[:5]!='Error':\n",
    "                            f=False\n",
    "                            #if(kdrow['filetype']==row['filetype'] or (kdrow['filetype']=='NOTIFICATION' and row['filetype'] in ['OTHER','NOTIFICATION']) )and (kdrow['purpose']=='CLASSIFICATION'):\n",
    "                            if(kdrow['filetype']==row['filetype'] or(kdrow['filetype']=='NOTIFICATION' and row['filetype']=='OTHER') )and (kdrow['purpose']=='CLASSIFICATION'):\n",
    "                                f=True\n",
    "                                for k in kdrow['keyword']: \n",
    "                                    kw = unidecode.unidecode(k)\n",
    "                                    \n",
    "\n",
    "                                    if (len(kw) < 5) or any(sp in kw.lower() for sp in special_keywords ):\n",
    "                                        if not (re.search(r'\\b' + kw + r'\\b', text)):\n",
    "                                            f=False\n",
    "                                    else:\n",
    "                                        match=[]\n",
    "                                        try:\n",
    "                                            match=find_near_matches(kw.lower(), text, max_l_dist=1, max_insertions=2)\n",
    "                                        except Exception as e:\n",
    "                                            f=False\n",
    "                                        if len(match)==0:\n",
    "                                            f=False\n",
    "                                        else:\n",
    "                                            f=True\n",
    "                            if f:\n",
    "                                fdf.loc[j,'pred_class'].append(get_predclass_fuzzy(kdf,kdrow,j,text,fdf))\n",
    "                                if kdrow['fileclass'] in list(fdf.loc[j,'keywords'].keys()):\n",
    "                                    fdf.loc[j,'keywords'][kdrow['fileclass']].append(kdrow['keyword'])\n",
    "                                else:\n",
    "                                    fdf.loc[j,'keywords'][kdrow['fileclass']]=list()\n",
    "                                    fdf.loc[j,'keywords'][kdrow['fileclass']].append(kdrow['keyword'])\n",
    "                fdf.set_value(j,'pred_class',list(set(fdf.loc[j,'pred_class'])))\n",
    "                N8_kw_to_desregard_extraction = 'las medidas ejecutivas concretas que resultaron procedentes'\n",
    "                for si,sr in suspkdf.iterrows():\n",
    "                    if not ((sr.remove_class=='N8' or sr.remove_class=='N15' or sr.remove_class=='N16')and(N8_kw_to_desregard_extraction in text)):\n",
    "                        f=True\n",
    "                        for sk in sr['keyword']:\n",
    "                            if unidecode.unidecode(sk) == 'dar cuenta a s.sa':\n",
    "                                if not (''.join('dar cuenta a s.s'.split()).lower() in ''.join(text.split() )):\n",
    "                                    f=False\n",
    "                            else:\n",
    "                                if not (''.join(unidecode.unidecode(sk).split()).lower() in ''.join(text.split())):\n",
    "                                    f=False\n",
    "                        if f and (row['filetype']=='NOTIFICATION' or row['filetype']=='OTHER'):\n",
    "                            if  not ((sr['remove_class']=='N2' or sr['remove_class']=='N12')and(\"SE ALZA LA SUSPENSION DE LAS ACTUACIONES\".lower() in text)):\n",
    "                                N2_kw_to_desregard_extraction = 'ACORDARA LA SUSPENSION DE LAS ACTUACIONES'\n",
    "                                                                # N2 extraction keyword\n",
    "                                if not ((sr.keyword[0]=='SUSPENSION DE LAS ACTUACIONES'.lower()) and (N2_kw_to_desregard_extraction.lower() in text)): \n",
    "                                    if 'NX-'+sr['remove_class'] in list(fdf.loc[j,'keywords'].keys()):\n",
    "                                        fdf.loc[j,'keywords']['NX-'+sr['remove_class']].append(sr['keyword'])\n",
    "                                    else:\n",
    "                                        fdf.loc[j,'keywords']['NX-'+sr['remove_class']]=list()\n",
    "                                        fdf.loc[j,'keywords']['NX-'+sr['remove_class']].append(sr['keyword'])\n",
    "                                    fdf.loc[j,'remove_class'].append(sr['remove_class'])\n",
    "\n",
    "                f=True\n",
    "                fdf.set_value(j,'remove_class',list(set(fdf.loc[j,'remove_class'])))\n",
    "                if not 'N-ALL' in fdf.loc[j,'remove_class']:\n",
    "                    for cl in list(set(fdf.loc[j,'pred_class'])-set(fdf.loc[j,'remove_class'])):\n",
    "                        fdf.loc[j,'after_classfn'].append(cl)\n",
    "                \n",
    "                if ('N1' in fdf.loc[j,'after_classfn']) and ('N5' in fdf.loc[j,'after_classfn']):\n",
    "                    N1_kw = 'admitir a tramite'\n",
    "                    partial_text = text[text.find(N1_kw)-3:]\n",
    "                    if partial_text.startswith('no'):\n",
    "                        fdf.loc[j,'after_classfn'].remove('N1')\n",
    "\n",
    "                for cla in list(fdf.loc[j,'after_classfn'] ): \n",
    "                    s=set(notification_corelation_dict[cla])\n",
    "                    if fdf['filename'] == \"2005_0000025_ETJ_20181023071050320180928135647_064_0061772_2018_001_0SjVdzAjCU.pdf\":\n",
    "                        print(set(row['after_classfn']),\"s===>\",s,set(row['after_classfn'])<=(s),set([cla])<=set(row['after_classfn']))\n",
    "                    if not(set(row['after_classfn'])<=(s) and set([cla])<=set(row['after_classfn'])):\n",
    "                        f=False\n",
    "                #print(\"f=\",f)\n",
    "                if f:\n",
    "                    fdf.set_value(j,'final_categ',list(set(fdf.loc[j,'after_classfn'])))\n",
    "                if 'N1' in fdf.loc[j,'final_categ'] or 'N5' in fdf.loc[j,'final_categ']:\n",
    "                    fl=False\n",
    "                    for flnx in NX_filename_N1_N5:\n",
    "                        if flnx in fdf.loc[j,'filename'].split('_')[2]:\n",
    "                            fl=True\n",
    "                    if fl:\n",
    "                        if 'N1' in fdf.loc[j,'final_categ']:\n",
    "                            fdf.loc[j,'final_categ'].remove('N1')\n",
    "                        elif 'N5' in fdf.loc[j,'final_categ']:\n",
    "                            fdf.loc[j,'final_categ'].remove('N5')\n",
    "\n",
    "    fgs=fdf.groupby('filegroup')\n",
    "    for k,v in list(fgs.groups.items()):\n",
    "        f=False\n",
    "        otherclass=\"N16\"\n",
    "        for ind in v:\n",
    "            if 'N16' in fdf.loc[ind,'final_categ']:\n",
    "                f=True\n",
    "            if 'N8' in fdf.loc[ind,'final_categ']:\n",
    "                otherclass='N8'\n",
    "            elif 'N15' in fdf.loc[ind,'final_categ']:\n",
    "                otherclass='N15'\n",
    "        for ind in v:\n",
    "            if f:\n",
    "                for n, i in enumerate(fdf.loc[ind,'final_categ']):\n",
    "                    if i == 'N16':\n",
    "                        fdf.loc[ind,'final_categ'][n] = otherclass\n",
    "\n",
    "    return fdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_accuraacy_percent(fdf,correct_df):\n",
    "def get_accuracy(fdf,correct_df, sam):\n",
    "    correct_df['correct_classification']=correct_df['correct_class'].apply(lambda x : '[\"NX\"]' if x!=x else x)\n",
    "    correct_df['correct_classification']=correct_df['correct_classification'].apply(lambda x : json.loads(x))\n",
    "\n",
    "    accu=0\n",
    "    total=0\n",
    "    fgs=fdf.groupby('filegroup')\n",
    "\n",
    "    for i,r in correct_df.iterrows():\n",
    "\n",
    "        if len(fdf.loc[fdf['filename']==r['filename']].values)>0:\n",
    "            fdf.loc[fdf['filename']==r['filename'],'correct_class']=json.dumps(r['correct_classification'])\n",
    "        pred=[]\n",
    "        if len(fdf.loc[fdf['filename']==r['filename'],'final_categ'].values)>0:\n",
    "            pred=fdf.loc[fdf['filename']==r['filename'],'final_categ'].values[0]\n",
    "        if len(pred)==0 and r['correct_classification'][0]=='NX':\n",
    "            accu+=1\n",
    "            total+=1\n",
    "            fdf.loc[fdf['filename']==r['filename'],'accuracy_index']=1\n",
    "        else:\n",
    "            if len(fdf.loc[fdf['filename']==r['filename'],'filetype'].values)>0 and fdf.loc[fdf['filename']==r['filename'],'filetype'].values[0]=='NOTIFICATION':\n",
    "                total+=1\n",
    "                f=False\n",
    "                if  set(pred)<= set(r['correct_classification']) :\n",
    "                        f=True\n",
    "                if  set(r['correct_classification'])<= set(pred) :\n",
    "                        f=True\n",
    "                if len(pred)==0:\n",
    "                    f=False\n",
    "                if f:\n",
    "                    fdf.loc[fdf['filename']==r['filename'],'accuracy_index']=1\n",
    "                    accu+=1\n",
    "                else:\n",
    "                    fdf.loc[fdf['filename']==r['filename'],'accuracy_index']=0\n",
    "    print(sam, accu,total, float(accu)/float(total))\n",
    "    return fdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FileGroup Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filegroup_class(df, sam):\n",
    "    group_columns = ['filegroup','files','filetypes','predicted_classes', 'final_class']\n",
    "    fgdf=pd.DataFrame(columns=group_columns, dtype=object)\n",
    "    for i, r in df.iterrows():\n",
    "        if bool(r.keywords):\n",
    "            if not bool(r.final_categ):\n",
    "                df.at[1, 'final_categ'] = ['NX']\n",
    "\n",
    "#                 df.set_value(i, 'final_categ', str([str('NX')]))\n",
    "    for fg, idx in df.groupby(['filegroup']).groups.items():\n",
    "        tdf = df.iloc[idx]\n",
    "        i = len(fgdf)\n",
    "        fgdf.set_value(i, 'filegroup', fg)\n",
    "        fgdf.set_value(i, 'files', tdf.filename.values.tolist())\n",
    "        fgdf.set_value(i, 'filetypes', tdf.filetype.values.tolist())\n",
    "        predicted_classes = sum([i if type(i)==list else literal_eval(i) for i in tdf.final_categ.values.tolist()], [])\n",
    "        fgdf.set_value(i, 'predicted_classes', predicted_classes)\n",
    "        fdf = tdf.loc[(tdf.filetype != 'TICKET') & (tdf.filetype != 'CARATULA')]\n",
    "        selected_predicted_classes = sum([i if type(i)==list else literal_eval(i) for i in fdf.final_categ.values.tolist()], [])\n",
    "        final_class = []\n",
    "        if 'N8' in selected_predicted_classes:\n",
    "            final_class = str([str('N8')])\n",
    "        elif 'NOTIFICATION' in fdf.filetype.values:\n",
    "            nfc = fdf.loc[fdf.filetype=='NOTIFICATION'].final_categ.values.tolist()\n",
    "            if (len(nfc) != 0) and  not('NX' in nfc[0]) :\n",
    "                final_class = nfc[0]\n",
    "                \n",
    "        fgdf.set_value(i, 'final_class', final_class)\n",
    "        fgdf.to_excel('filegroup_class_sample'+str(sam)+'.xlsx')\n",
    "        \n",
    "    return fgdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_fdf1 = pd.read_excel('/home/thrymr/552-python-Workspace/ocr ang⁄flask/latest_test/sample_1_parsed_data.xlsx')\n",
    "parsed_fdf2 = pd.read_excel('/home/thrymr/552-python-Workspace/ocr ang⁄flask/latest_test/sample_2_parsed_data.xlsx')\n",
    "parsed_fdf3 = pd.read_excel('/home/thrymr/552-python-Workspace/ocr ang⁄flask/latest_test/sample_3_parsed_data.xlsx')\n",
    "parsed_fdf4 = pd.read_excel('/home/thrymr/552-python-Workspace/ocr ang⁄flask/latest_test/sample_4_parsed_data.xlsx')\n",
    "parsed_fdf5 = pd.read_excel('/home/thrymr/552-python-Workspace/ocr ang⁄flask/latest_test/sample_5_parsed_data.xlsx')\n",
    "parsed_fdf6 = pd.read_excel('/home/thrymr/552-python-Workspace/ocr ang⁄flask/latest_test/sample_6_parsed_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fdf_fromfile_pathand_save_excel(fdf,kdf,suspkdf,notification_corelation_dict ,sam,correct_df): #\n",
    "    fdf=get_classify_result(fdf,kdf,suspkdf,notification_corelation_dict)\n",
    "    #fdf =get_accuracy(fdf.copy(),correct_df.copy(), sam)\n",
    "    return fdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf = pd.read_excel(\"/home/thrymr/552-python-Workspace/shashank_classfication/upload_N18_parse_26_10.xlsx\")\n",
    "#f =\"2007_0000844_ENJ_20181023200382820181005100331_043_070264200100000183392018070264200111.PDF\"\n",
    "#fdf = fdf.loc[fdf['filename']==f]\n",
    "fdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:138: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:163: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:181: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "fdf = get_fdf_fromfile_pathand_save_excel(fdf,kdf,suspkdf,notification_corelation_dict,'1',\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.loc[fdf['filename']==\"2014_0000018_ENJ_20181023300913920181010190144_084_8867435_2018_I_174961916.PDF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.loc[(fdf['filetype']!= 'TICKET')& (fdf['filetype']!= 'CARATULA')].to_excel(\"/home/thrymr/552-python-Workspace/shashank_classfication/results_N18_classification_26_10.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.loc[(fdf['filetype']!= 'TICKET')].to_excel(\"/home/thrymr/552-python-Workspace/shashank_classfication/results_N17_classification2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.loc[fdf['filetype']!= 'TICKET'].to_excel(\"/home/thrymr/552-python-Workspace/shashank_classfication/results_N17_classification.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fdf1=get_fdf_fromfile_pathand_save_excel(parsed_fdf1,kdf,suspkdf,notification_corelation_dict,'1',correct_class_dfs['Sample_1'].copy())\n",
    "fdf2=get_fdf_fromfile_pathand_save_excel(parsed_fdf2,kdf,suspkdf,notification_corelation_dict,'2',correct_class_dfs['Sample_2'].copy())\n",
    "fdf3=get_fdf_fromfile_pathand_save_excel(parsed_fdf3,kdf,suspkdf,notification_corelation_dict,'3',correct_class_dfs['Sample_3'].copy())\n",
    "fdf4=get_fdf_fromfile_pathand_save_excel(parsed_fdf4,kdf,suspkdf,notification_corelation_dict,'4',correct_class_dfs['Sample_4'].copy())\n",
    "fdf5=get_fdf_fromfile_pathand_save_excel(parsed_fdf5,kdf,suspkdf,notification_corelation_dict,'5',correct_class_dfs['Sample_5'].copy())\n",
    "fdf6=get_fdf_fromfile_pathand_save_excel(parsed_fdf6,kdf,suspkdf,notification_corelation_dict,'6',correct_class_dfs['Sample_1'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf6[['filename','filegroup','filetype','keywords','final_categ']].to_excel('sample6_classification.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fdf1.loc[(fdf1.accuracy_index == 1)&(fdf1.filetype=='NOTIFICATION')])/len(fdf1.loc[fdf1.filetype=='NOTIFICATION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_files1 = fdf1.loc[(fdf1['accuracy_index'] == 0)&(fdf1.filetype == 'NOTIFICATION')]\n",
    "errors_files2 = fdf2.loc[(fdf2['accuracy_index'] == 0)&(fdf2.filetype == 'NOTIFICATION')]\n",
    "errors_files3 = fdf3.loc[(fdf3['accuracy_index'] == 0)&(fdf3.filetype == 'NOTIFICATION')]\n",
    "errors_files4 = fdf4.loc[(fdf4['accuracy_index'] == 0)&(fdf4.filetype == 'NOTIFICATION')]\n",
    "errors_files5 = fdf5.loc[(fdf5['accuracy_index'] == 0)&(fdf5.filetype == 'NOTIFICATION')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_files1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_files1[['filename','filegroup','filetype','keywords','final_categ', 'correct_class']].to_excel('sample1_error_cases.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_files1.filename.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '2008_0001021_ENJ_20181021021551520180521142819_03_0015100_2018_001_u4fR19z8sS.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf4.loc[fdf4.filename == fn].keywords.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf4.loc[fdf4.filename == fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = fdf4.loc[fdf4.filename == fn].text_response.values[0].lower()\n",
    "\n",
    "text = re.sub(' +',' ', t)\n",
    "text = text.replace('\\n', '')\n",
    "text = text.replace('\\xaa', '')\n",
    "paratlist=['MODO DE IMPUGNACION:', 'mode d\\'impugnacio', 'recurso de repelacion',\n",
    "           'recurs de reposicio', 'recurso de reposicion', 'recurso de apelacion', \n",
    "           'INTERPONER RECURSO DIRECTO DE REVISION']\n",
    "rem=''\n",
    "for parat in paratlist:\n",
    "\n",
    "    rem_word = parat.lower()\n",
    "    if (rem_word in text):\n",
    "        rem=text.split(rem_word)[-1]\n",
    "    text=text.replace(rem,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_keywords = ['facilite','despachar','admitir','oigase', '2 copias', 'averiguar', 'requerir', 'requeriu']\n",
    "for i,kdrow in kdf.loc[(kdf.purpose == 'CLASSIFICATION')&(kdf.filetype=='NOTIFICATION')].iterrows():\n",
    "    f=True\n",
    "    for k in kdrow['keyword']: \n",
    "        kw = unidecode.unidecode(k)\n",
    "        if (len(kw) < 5) or any(sp in kw.lower() for sp in special_keywords ):\n",
    "            if not (re.search(r'\\b' + kw + r'\\b', text)):\n",
    "                f=False\n",
    "        else:\n",
    "            match = find_near_matches(kw.lower(), text, max_l_dist=1)\n",
    "            if len(match) == 0:\n",
    "                f = False\n",
    "    if f:\n",
    "        print(kdrow.fileclass, kdrow.keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for si,sr in suspkdf.iterrows():\n",
    "    f=True\n",
    "    for sk in sr['keyword']:\n",
    "        if not (''.join(unidecode.unidecode(sk).split()).lower() in ''.join(text.split() )):\n",
    "            f=False\n",
    "    if f:\n",
    "        print(sr.remove_class, sr.keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= {'a':'hi'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suplico_parsed = pd.read_excel('/home/thrymr/552-python-Workspace/ocr ang⁄flask/latest_test/suplico_parsed_data.xlsx')\n",
    "suplico_fdf=get_fdf_fromfile_pathand_save_excel(suplico_parsed,kdf,suspkdf,notification_corelation_dict,'5',correct_class_dfs['Sample_5'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suplico_fdf.filename.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suplico_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf6[['filename','filegroup','filetype','final_categ']].to_excel('sample6_classfication_result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgdf6 = filegroup_class(fdf6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = '20181021739487020180626082320'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf6.loc[fdf6.filegroup == fg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf6.loc[fdf6.filegroup == fg].filename.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_near_matches('PATTERN', '--- PATVERNNN ---', max_deletions=1, max_insertions=1, max_substitutions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgdf6.to_excel('sample_6_filegroup_class.xlsx')[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf6.loc[0, 'final_categ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(kdrow['filetype']==row['filetype'] or (kdrow['filetype']=='NOTIFICATION' and row['filetype'] in ['OTHER','NOTIFICATION']) )and (kdrow['purpose']=='CLASSIFICATION'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf1.loc[fdf1['accuracy_index'] == 0].to_excel('sample1_test_failed_cases.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['filename', 'filegroup', 'filetype', 'text_response', 'keywords', 'final_categ', 'correct_class']].filename.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 450 464 0.9698275862068966"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fdf,fgdf=get_fdf_fromfile_pathand_save_excel(\"/home/thrymr/Racmo/processed/notification_samples_0904_02\",kdf,suspkdf,notification_corelation_dict,'1',correct_class_dfs['Sample_1'].copy())\n",
    "fdf2,fgdf2=get_fdf_fromfile_pathand_save_excel(\"/home/thrymr/Racmo/processed/notification_samples_0905_2_v2\",kdf,suspkdf,notification_corelation_dict,'2',correct_class_dfs['Sample_2'].copy())\n",
    "fdf3,fgdf3=get_fdf_fromfile_pathand_save_excel(\"/home/thrymr/Racmo/processed/new_category_tests\",kdf,suspkdf,notification_corelation_dict,'3',correct_class_dfs['Sample_3'].copy())\n",
    "fdf4,fgdf4=get_fdf_fromfile_pathand_save_excel(\"/home/thrymr/Racmo/processed/Week_2/notification_samples_2205_v1\",kdf,suspkdf,notification_corelation_dict,'4',correct_class_dfs['Sample_4'].copy())\n",
    "fdf5,fgdf5=get_fdf_fromfile_pathand_save_excel(\"/home/thrymr/Racmo/processed/Week_3\",kdf,suspkdf,notification_corelation_dict,'5',correct_class_dfs['Sample_5'].copy())\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
