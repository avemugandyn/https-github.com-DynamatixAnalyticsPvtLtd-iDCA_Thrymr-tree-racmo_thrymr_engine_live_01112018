{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdfquery\n",
    "import json\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import cascaded_union\n",
    "import pdftableextract as pte\n",
    "from math import floor\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import hashlib\n",
    "import unidecode\n",
    "from fuzzysearch import find_near_matches\n",
    "import textract\n",
    "import unicodedata\n",
    "from pymongo import MongoClient\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "#from configuration.configuration import ConfigClass,DbConf\n",
    "import shutil\n",
    "import datetime\n",
    "import multiprocessing as mp\n",
    "import datefinder\n",
    "import zipfile\n",
    "import signal\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PDF_DIR = \"\"\n",
    "temp_fdf = pd.DataFrame()\n",
    "classi_fdf = pd.DataFrame()\n",
    "extract_fdf = pd.DataFrame()\n",
    "\n",
    "#class Document_Analysis:\n",
    "    \n",
    "# SELECT max(batch_id) FROM file_classification;\n",
    "#     def keywordimport():\n",
    "#         engine = create_engine(ConfigClass.SQLALCHEMY_DATABASE_URI)\n",
    "        \n",
    "#         susp_df= pd.read_sql_query('SELECT k.id,k.file_class as fileclass,k.file_type as filetype,\\\n",
    "#                                    k.keyword,k.remove_class FROM suspend_keywords k',engine)\n",
    "#         keyword_df= pd.read_sql_query('SELECT k.id,k.file_class as fileclass,k.file_type as filetype,k.purpose,\\\n",
    "#                                     k.decision_type,k.keyword,k.bias as bias,k.sub as sub FROM keywords k',engine)\n",
    "#         keyword_df['sub']=keyword_df['sub'].apply(lambda x : json.loads(x) if x!=None else [])\n",
    "#         keyword_df['keyword']=keyword_df['keyword'].apply(lambda x : json.loads(x))\n",
    "\n",
    "#         return keyword_df, susp_df\n",
    "\n",
    "#Getting number of pages in a pdf file    \n",
    "#     def get_pgnum(filename):\n",
    "#         pdf = pdfquery.PDFQuery(ConfigClass.UPLOAD_FOLDER + \"/\" + filename)\n",
    "#         pdf.load()\n",
    "#         pgn = len(pdf.tree.getroot().getchildren())\n",
    "#         return pgn\n",
    "\n",
    " #Takes a dataframe of filegroups which also contains the list of files as its argument and returns a dataframe files with columns, name,file group and its filetype(right now its either ticket or other as the file with smallest name in a filegroup is ticket)\n",
    "def get_structured_files_dataframe(df):\n",
    "    tmp = pd.DataFrame(columns=[\"file\",\"group\"])\n",
    "    j = 0\n",
    "    for i, r in df.iterrows():\n",
    "        for f in r.files:\n",
    "            tmp.loc[j] = [f, r.group]\n",
    "            j = j+1\n",
    "    tmp[\"ext\"] = tmp.apply(lambda x : x.file.split(\".\")[-1],axis =1)\n",
    "    tmp[\"length\"] = tmp.apply(lambda x : len(x.file),axis =1)\n",
    "    TI, OI = [], []\n",
    "    for case, indices in list(tmp.groupby(\"group\").groups.items()):\n",
    "        ticket_index = tmp.loc[indices].length.values.argmin()\n",
    "        for i, idx in enumerate(indices):\n",
    "            if i == ticket_index:\n",
    "                TI.append(idx)\n",
    "            else:\n",
    "                OI.append(idx)\n",
    "    tmp['type'] = \"OTHER\"\n",
    "    tmp.loc[TI,\"type\"] = \"TICKET\"\n",
    "    return tmp\n",
    "\n",
    "    #takes a df and returns to json, the df is taken from the data extracted from the tables of pdfs using pdf tables extract\n",
    "def df_to_json(df):\n",
    "    js = {}\n",
    "    vals = df[0].values\n",
    "    ls = []\n",
    "    for i, v in enumerate(vals):\n",
    "        if not v:\n",
    "            ls.append(ls[-1])\n",
    "        else:\n",
    "            ls.append(v)\n",
    "    df[0] = ls\n",
    "    for key, idx in list(df.groupby([0]).groups.items()):\n",
    "        js[key] = {}\n",
    "        for val in df.loc[idx][[1,2]].values:\n",
    "            if val[0] == \"\" and val[1] == \"\":\n",
    "                js[key][\"_value\"] = \"\"\n",
    "            elif val[0] == \"\" and val[1] != \"\":\n",
    "                js[key][\"_value\"] = val[1]\n",
    "            elif val[0] != \"\" and val[1] == \"\":\n",
    "                js[key][\"_value\"] = val[0]\n",
    "            elif val[0] != \"\" and val[1] != \"\":\n",
    "                js[key][val[0]] = val[1]                \n",
    "        if(key==\"Documentos\"):\n",
    "            js[key] = df.loc[idx][[1,2]].values.tolist()\n",
    "        if len(js[key]) == 1 and \"_value\" in js[key]:\n",
    "            js[key] = js[key][\"_value\"]\n",
    "    return js\n",
    "\n",
    "    #takes the path of the pdf and returns its text using pdf query the text extracted will be sorted accoring to its y cooridinates of its bounding boxes\n",
    "def get_pdf_text(path):\n",
    "    try:\n",
    "        pdf=pdfquery.PDFQuery(path)\n",
    "        pdf.load()\n",
    "        pdftext=\"\"\n",
    "        pgn=len(pdf.tree.getroot().getchildren())\n",
    "        for i in range(0,pgn):\n",
    "            root = pdf.tree.getroot().getchildren()[i]\n",
    "            npg=[]\n",
    "            for node in root.iter():\n",
    "                try:\n",
    "        #             if node.tag == \"LTTextLineHorizontal\" or node.tag == \"LTTextBox\" or node.tag==\"LTTextBoxHorizontal\":\n",
    "                    if node.text and float(node.get(\"y1\"))>50 and float(node.get(\"x1\"))>50:\n",
    "                        npg.append(node)\n",
    "        #                     pdftext=pdftext+\"\\n\"\n",
    "                except Exception as e:\n",
    "                    print((node.tag, e))\n",
    "            npg=sorted(npg, key=lambda x: float(x.get(\"x0\")))\n",
    "            npg=sorted(npg, key=lambda x: round(float(x.get(\"y0\"))),reverse=True)\n",
    "            if len(npg)>0:\n",
    "                prev_y=round(float(npg[0].get(\"y0\")))\n",
    "                for x in npg: \n",
    "                    esc=\"\\n\"\n",
    "                    if round(float(x.get(\"y0\")))==prev_y:\n",
    "                        esc=\"|\"\n",
    "    #                         print(x.get(\"y0\"),x.get(\"y1\"),x.text)\n",
    "                    prev_y=round(float(x.get(\"y0\")))\n",
    "                    if len(x.text)> 0:\n",
    "                        pdftext+=esc+x.text\n",
    "        return pdftext\n",
    "    except Exception as e:\n",
    "        return('Error:'+str(e))\n",
    "\n",
    " #takes rtf files path and returns its text    \n",
    "def get_rtf_text(path):\n",
    "    text = os.popen('unrtf --text '+path).read()\n",
    "    return text\n",
    "\n",
    "    #takes the path of a pdf and extract table 1 and table 2 of tickets and returns its json \n",
    "def parse_ticket(pdf_path):\n",
    "    _JSON = {\"filepath\" : pdf_path}\n",
    "    try:\n",
    "        pdf = pdfquery.PDFQuery(pdf_path)\n",
    "        pdf.load()\n",
    "        root = pdf.tree.getroot().getchildren()[0]\n",
    "        page_box = [float(x) for x in root.get(\"bbox\")[1:-1].split(\",\")]\n",
    "        tables, _ =\\\n",
    "        list(zip(\n",
    "            *sorted(\n",
    "                [(p.bounds,p.area) for p in cascaded_union(\n",
    "                    [box(*[float(x) for x in node.get(\"bbox\")[1:-1].split(\",\")]) for node in root.iter() if node.tag == \"LTRect\"]\n",
    "                )],\n",
    "                key = lambda x : -x[1]\n",
    "            )\n",
    "        ))\n",
    "        X = page_box[2]\n",
    "        Y = page_box[3]\n",
    "        xf = 11.69/X\n",
    "        yf = 8.27/Y\n",
    "        t1, t2 = tables\n",
    "\n",
    "        table_1_bbox = \":\".join(map(str,(t1[0]*xf - 0.1, (Y - t1[3])*yf - 0.1, t1[2]*xf + 0.1, (Y - t1[1])*yf + 0.1)))\n",
    "        table_2_bbox = \":\".join(map(str,(t2[0]*xf - 0.1, (Y - t2[3])*yf - 0.1, t2[2]*xf + 0.1, (Y - t2[1])*yf + 0.1)))\n",
    "\n",
    "        df1 =\\\n",
    "        pd.DataFrame(\n",
    "            pte.table_to_list(\n",
    "                pte.process_page(\n",
    "                    pdf_path,\n",
    "                    \"1\",\n",
    "                    crop = table_1_bbox,\n",
    "                    pad=20\n",
    "                ),\n",
    "                \"1\"\n",
    "            )[1]\n",
    "        )\n",
    "        _JSON[\"table_1\"] = df_to_json(df1)\n",
    "        df2 = \\\n",
    "        pd.DataFrame(\n",
    "            pte.table_to_list(\n",
    "                pte.process_page(\n",
    "                    pdf_path,\n",
    "                    \"1\",\n",
    "                    crop = table_2_bbox,\n",
    "                    pad=20\n",
    "                ),\n",
    "                \"1\"\n",
    "            )[1]\n",
    "        )\n",
    "        df2.columns = df2.iloc[0]\n",
    "        df2 = df2.reindex(df2.index.drop(0))\n",
    "        _JSON[\"table_2\"] = df2.to_json(orient='index')\n",
    "    except Exception as e:\n",
    "            return('Error:'+str(e))\n",
    "    return json.dumps(_JSON, ensure_ascii=False)\n",
    "\n",
    "    #takes a path of a file(pdf and rtf) and extract its texts and remove its accents of spansish characters\n",
    "def parse_other(pf):\n",
    "    text=\"\"\n",
    "    try:\n",
    "        if pf[-3:].lower()=='rtf':\n",
    "            #print('rtf')\n",
    "            newtext = unidecode.unidecode(textract.process(pf).decode('utf-8'))\n",
    "            #newtext =textract.process(pf)\n",
    "            #newtext=str(newtext,'utf-8')\n",
    "            # newtext=newtext\n",
    "        else:\n",
    "            newtext= get_pdf_text(pf)\n",
    "        if(len(newtext.split())==0):\n",
    "            #print(\"scan\")\n",
    "            newtext =textract.process(pf,method='tesseract').decode('utf-8')\n",
    "        try:\n",
    "            newtext=(''.join((c for c in unicodedata.normalize('NFD', newtext) if unicodedata.category(c) != 'Mn'))).lower()\n",
    "        except Exception as e:\n",
    "            print((str(e)+\" binary \",pf[-3:].lower()))\n",
    "            newtext=(''.join((c for c in unicodedata.normalize('NFD', newtext.decode(\"utf-8\")) if unicodedata.category(c) != 'Mn'))).lower()\n",
    "        rem=''\n",
    "        paratlist=['MODO DE IMPUGNACION:'.lower(),'mode d\\'impugnacio',\n",
    "                   'recurso de repelacion','recurs de reposicio','recurso de reposicion','recurso de apelacion',\n",
    "                    'INTERPONER RECURSO DIRECTO DE REVISION']\n",
    "\n",
    "        for parat in paratlist:\n",
    "            if (parat.lower() in newtext) :\n",
    "                rem=newtext.split(parat.lower())[-1]\n",
    "        newtext=newtext.replace(rem,'')\n",
    "    except Exception as e:\n",
    "        newtext='Error:'+str(e)\n",
    "    return newtext\n",
    "\n",
    "def unzip_add(fdf,PDF_DIR):\n",
    "    ffdf = fdf.copy()\n",
    "    #import zipfile\n",
    "    try:\n",
    "        for i, r in fdf.iterrows():\n",
    "            zip_dict ={}\n",
    "            if r.filename[-3:].lower()=='zip':\n",
    "                z_files = []\n",
    "                #print((r.filename.split('.')[0]))\n",
    "                with zipfile.ZipFile(join(PDF_DIR,r.filename)) as z:\n",
    "                    for fileinzip in [x for x in z.namelist()]:\n",
    "                        if not os.path.isdir(fileinzip):\n",
    "                            try:\n",
    "                                #fileinzip = unidecode.unidecode(fileinzip)\n",
    "                                zfdir=join(PDF_DIR, os.path.basename(fileinzip))\n",
    "                            except Exception as e:\n",
    "                                fileinzip = unidecode.unidecode(fileinzip)\n",
    "                                zfdir=join(PDF_DIR, os.path.basename(fileinzip))\n",
    "                            try:\n",
    "                                with z.open(os.path.join(fileinzip)) as fz,open(zfdir, 'wb') as zfp:\n",
    "                                    shutil.copyfileobj(fz, zfp)\n",
    "                                    #os.remove(zfdir)\n",
    "                                    if fileinzip.count('.') > 1:\n",
    "                                        condition = '.' in fileinzip\n",
    "                                        while(condition):\n",
    "                                            ind = fileinzip.find('.')\n",
    "                                            if ind != fileinzip.rfind('.'):\n",
    "                                                fileinzip = fileinzip[0:ind] + '_' + fileinzip[ind+1:]\n",
    "                                            else:\n",
    "                                                condition = False\n",
    "\n",
    "                                    f_name = r.filename[:-4]+'_'+str(''.join(fileinzip.split()))\n",
    "                                    f_name = f_name.replace('/','_')\n",
    "                                    if r.filename[:-4] in f_name:\n",
    "                                        if not os.path.exists(join(PDF_DIR,f_name)) and not os.path.exists(join(PDF_DIR,unidecode.unidecode(f_name))):\n",
    "                                            shutil.move(zfdir, join(PDF_DIR,unidecode.unidecode(f_name)))\n",
    "                                            z_files.append({\"filename\":unidecode.unidecode(f_name),\"filegroup\":r['filegroup'],\\\n",
    "                                                \"filetype\":\"OTHER\",'ext':fileinzip[-3:],\"length\":len(fileinzip)})\n",
    "                                        else:\n",
    "                                            os.remove(zfdir)\n",
    "                            except Exception as e:\n",
    "                                 print(e)\n",
    "                zip_dict[r['filegroup']]= z_files                                                                  \n",
    "            if zip_dict !={}:\n",
    "                for k,val in list(zip_dict.items()):\n",
    "                    for v in val:\n",
    "                        ffdf = ffdf.append(pd.Series(v), ignore_index=True) \n",
    "    except Exception as e:\n",
    "        print(str(e))           \n",
    "    return ffdf         \n",
    "       \n",
    "\n",
    "#takes file data frame and returns its table response(table json) and text response\n",
    "def parsefile(file_name):\n",
    "    global PDF_DIR\n",
    "    global temp_fdf\n",
    "    def timeout(signum, frame):\n",
    "        raise ValueError('time out,a very specific bad thing happened.')\n",
    "    signal.signal(signal.SIGALRM, timeout)\n",
    "    df = temp_fdf.loc[temp_fdf['filename']==file_name]\n",
    "    zips_filegroup = {}\n",
    "    for i, r in df.iterrows():\n",
    "        ticresponse=\"\"\n",
    "        textresponse=\"\"\n",
    "        zip_dict = []\n",
    "        try:\n",
    "            signal.alarm(45)\n",
    "            while 1:\n",
    "                ticresponse= parse_ticket(join(PDF_DIR,r.filename))\n",
    "                if len(ticresponse)>5:\n",
    "                    signal.alarm(0)\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            ticresponse='Error:'+str(e)\n",
    "        try:\n",
    "            signal.alarm(45)\n",
    "            if r.filename[-3:].lower()!='zip':\n",
    "                while 1:\n",
    "                    if r.filetype=='TICKET':\n",
    "                        textresponse=ticresponse\n",
    "                    else:\n",
    "                        textresponse= parse_other(join(PDF_DIR,r.filename))\n",
    "                        if 'Error:' in textresponse:\n",
    "                            print((\"text\",i,textresponse))\n",
    "                    if len(ticresponse)>5:\n",
    "                        signal.alarm(0)\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            textresponse='Error:'+str(e)\n",
    "            if (r.filename[-3:].lower()=='rtf'):\n",
    "                print(\"___________________________________________________\")\n",
    "\n",
    "        df.loc[i,\"table_response\"] = ticresponse\n",
    "        df.loc[i,\"text_response\"] = textresponse\n",
    "    return df\n",
    "\n",
    "    # Once the table json is extracted we know which is the principal notification file and moves on to update it\n",
    "def update_filetype(fdf):\n",
    "    fgs=fdf.groupby('filegroup')\n",
    "    fgdf=pd.DataFrame(columns=['filegroup'])\n",
    "    i=0\n",
    "    fgdf['files']=np.empty((len(fgs.groups), 0)).tolist()\n",
    "    fgdf['filetypes']=np.empty((len(fgs.groups), 0)).tolist()\n",
    "    for k,v in list(fgs.groups.items()):\n",
    "        fgdf.loc[i,'filegroup']=k\n",
    "        files=[]\n",
    "        pcs=[]\n",
    "        filetypes=[]\n",
    "        for ind in v:\n",
    "                files.append(fdf.loc[ind,'filename'])\n",
    "                filetypes.append(fdf.loc[ind,'filetype'])\n",
    "        fgdf.loc[i,'filetypes']=filetypes\n",
    "        fgdf.loc[i,'files']=files\n",
    "        i+=1\n",
    "    for i ,r in fdf[~(fdf['table_response'].str.contains('Error:'))&(fdf['filetype']=='TICKET')].iterrows():\n",
    "        if r['table_response'][:5]!='Error':\n",
    "            js=json.loads(r['table_response'])\n",
    "            pf=r['filename'].split('.')[0]+'_'+''.join(js['table_1']['Documentos'][0][0].split()).split('(Principal)')[0]\n",
    "            fgf=[''.join(x.split())for x in fgdf.loc[fgdf['filegroup']==r['filegroup'],'files'].values[0] ]\n",
    "            if pf in fgf:\n",
    "                fl=fgdf.loc[fgdf['filegroup']==r['filegroup'],'files'].values[0][fgf.index(pf)]\n",
    "                if \"caratula\" in pf.lower():\n",
    "                    fdf.loc[fdf['filename'].str.contains(fl),'filetype']=\"CARATULA\"\n",
    "                else:\n",
    "                    fdf.loc[fdf['filename'].str.contains(fl),'filetype']=\"NOTIFICATION\"\n",
    "\n",
    "    return fdf,fgdf \n",
    "\n",
    "def read_pdf_n_insert(root_new):\n",
    "    global PDF_DIR\n",
    "    global temp_fdf\n",
    "    global classi_fdf\n",
    "    global extract_fdf\n",
    "    file_nm_error = list()\n",
    "\n",
    "    t_time = time.time()\n",
    "    PDF_DIR = root_new\n",
    "    pdf_files= [f for f in listdir(PDF_DIR) if isfile(join(PDF_DIR,  f))]\n",
    "    if len(pdf_files)>0:\n",
    "        ls=list()\n",
    "        for pdf_file in pdf_files:\n",
    "            if len(pdf_file.split('_'))>=4 and len(pdf_file.split('_')[2])<=4:\n",
    "                ls.append(pdf_file.split('_'))\n",
    "            else:\n",
    "                file_nm_error.append(pdf_file)\n",
    "\n",
    "        df=pd.DataFrame(ls)\n",
    "        df = df[pd.notnull(df[3])]\n",
    "        fg=list(df.groupby(3))\n",
    "        ls=[]\n",
    "        i=0\n",
    "        for k,gdf in  fg:\n",
    "            fglist=[]\n",
    "            elist=[]\n",
    "            flist=list()\n",
    "            for i, row in gdf.iterrows():\n",
    "                row=row.dropna()\n",
    "                fln='_'.join(list(row))\n",
    "                flist.append(fln)\n",
    "                elist.append(fln[-3:].lower())\n",
    "            fgroup={'group':k,'files':flist,'length':len(flist),'min_filename':min(flist, key=len),'extensions':elist}\n",
    "            ls.append(fgroup)\n",
    "        flgdf=pd.DataFrame(ls)\n",
    "        flgdf=flgdf.dropna(thresh=1,axis=1)\n",
    "        cmpt_fdf = get_structured_files_dataframe(flgdf)\n",
    "        cmpt_fdf= cmpt_fdf.rename(columns={\"file\":\"filename\",\"group\":\"filegroup\",\"type\":\"filetype\"})\n",
    "        cmpt_fdf = unzip_add(cmpt_fdf,PDF_DIR)\n",
    "        temp_fdf = cmpt_fdf\n",
    "        list_of_files = cmpt_fdf['filename'].values.tolist()\n",
    "        print(\"Parsing start\")\n",
    "        parsing_start_t = time.time()\n",
    "        pool = mp.Pool(processes=mp.cpu_count())\n",
    "        results = pool.map_async(parsefile,list_of_files)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        print(\"Parsing complete(time in minutes) = \",(float(time.time()-parsing_start_t)/60))\n",
    "        output = results.get()\n",
    "        final_fdf=pd.concat(output)\n",
    "        fdf,fgdf=update_filetype(final_fdf)\n",
    "    return fdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 13] Permission denied: '/home/racmo/upload_N27/174762389_Prov.  9.10.18 requerimiento a la A.C..RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N27/174520058_Prov. apercibimiento Ac presente informes trimestrales..RTF'\n",
      "[Errno 13] Permission denied: '/home/racmo/upload_N27/175756928_Prov. traslado alegaciones administrador concursal.RTF'\n",
      "Parsing start\n",
      "('text', 30, 'Error:time out,a very specific bad thing happened.')\n",
      "('text', 39, 'Error:time out,a very specific bad thing happened.')\n",
      "('text', 109, 'Error:time out,a very specific bad thing happened.')\n",
      "Parsing complete(time in minutes) =  4.292584685484568\n"
     ]
    }
   ],
   "source": [
    "fdf = read_pdf_n_insert(\"/home/racmo/upload_N27/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf.to_excel(\"/home/thrymr/552-python-Workspace/shashank_classfication/upload_N27_parse_30_10.xlsx\")\n",
    "#dc.read_pdf_n_insert()\n",
    "fdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf2 = read_pdf_n_insert(\"/home/racmo/upload_N19/\")\n",
    "fdf2.to_excel(\"/home/thrymr/552-python-Workspace/shashank_classfication/upload_N19_parse_25_10.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtext =textract.process('/home/racmo/upload_N17/2015_0005652_FIC_20181023080359020181001083011_030_16024_20180928_1000_0017367318_02.rtf')\n",
    "newtext = ' '.join(newtext.split()).replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = pd.read_excel(\"/home/thrymr/552-python-Workspace/shashank_classfication/upload_N27_parse_30_10.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N17_key = [unidecode.unidecode(x).lower() for x in ['DESE CUENTA A S.Sa','SE DA CUENTA A S.Sa','DAR CUENTA A S.Sa','ACUERDO DAR CUENTA A S.Sa','DESE CUENTA A LA MAGISTRADO JUEZ']]\n",
    "N17_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N24_key = ['acuerdo el archivo definitivo de las actuaciones','archivar el presente procedimiento','archivar las actuaciones',\n",
    " 'archivase el expediente','archivense los presentes autos','dando publicidad a la conclusion y archivo','dar por terminado el presente expediente',\n",
    " 'dar por terminado el presente procedimiento','declarar terminado el presente procedimiento de ejecucion','declaro finalizado el presente procedimiento monitorio',\n",
    " 'se acuerda dar por terminado el proceso monitorio','se acuerda el archivo de las actuaciones','se acuerda el archivo definitivo',\n",
    " 'se declara finalizado el presente proceso monitorio','se declara terminado','se decreta la terminacion del proceso','tener por terminado']\n",
    "N34_key = ['SE ACUERDA EL SOBRESEIMIENTO']\n",
    "N19_key = ['declarar embargados, por via de mejora de embargo','se acuerda la mejora del embargo','acuerdo la mejora del embargo',\n",
    "           \"disposo la millora de l'embargament\",'se declara embargado, por via de mejora','se decreta el embargo, por via de mejora',\n",
    "           'se declaran embargados, por via de mejora de embargo','se decrete la mejora de embargo',\n",
    "           'declarar embargada, por via de mejora de embargo','procedase a instancia']\n",
    "N36_key = ['se alza la suspension de las actuaciones']\n",
    "N27_key = [x for x in list(kdf.loc[(kdf.fileclass == 'N27')]['keyword'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N18_key = ['se decalaran embargados','parte proporcional de la pension que perciba','el embargo de los bienes propiedad del ejecutado',\n",
    " 'el embargo telematico de las cuentas','el embargo telematico de las devoluciones','declarar embargados los siguientes bienes',\n",
    " 'para la efectividad de los embargos, materialicense telematicamente desde este juzgado','declaro embargats','per assegurar la trava dels bens embargats',\n",
    " \"l'ordre d'embargament s'ha de cursar\",'la parte proporcional del salario que percibe','la parte proporcional de los salarios',\n",
    " 'devoluciones que los ejecutados tengan pendientes','para asegurar la traba de los bienes embargados','parte legal del salario que el ejecutado',\n",
    " 'se declara embargado','el embargo de los bienes','la practica de dichos embargos se debera realizar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fussy_match(k):\n",
    "    f= True\n",
    "    for kk in k:\n",
    "        match=find_near_matches(unidecode.unidecode(kk).lower(), text, max_l_dist=1, max_insertions=2)\n",
    "        if len(match) <= 0:\n",
    "            f= False\n",
    "            #print('\\\\n',match,len(match))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "jdo. de lo mercantil n. 1 de badajoz \n",
      " -  \n",
      "c/ castillo puebla de alcocer, 20 \n",
      "telefono: 924286421, fax: 924286455 \n",
      "equipo/usuario: irm \n",
      "modelo: s40040 \n",
      " n .i.g.: 06015 47 1 2012 0000034 \n",
      "s5c seccion v convenio 0000026 /2012 \n",
      "procedimiento origen: cno concurso ordinario 0000026 /2012 \n",
      "sobre otras materias \n",
      " demandante d/na. sistemas de automatismo y control sa \n",
      "procurador/a sr/a. jose luis riesco martinez \n",
      "abogado/a sr/a. \n",
      " d/na. \n",
      "procurador/a sr/a. \n",
      "demandantesistemas de automatismo y control sa d/na. abogado/a sr/a. \n",
      " d iligencia de ordenacion \n",
      " s ra. letrado de la administracion de justicia \n",
      "dna. lucia peces gomez \n",
      " e n badajoz, a diecinueve de octubre de dos mil dieciocho. \n",
      " p resentado el anterior escrito por el procuradora sr. \n",
      "fernandez de arevalo, en nombre y representacion de francisco \n",
      "garcia rangel, comunicando el impago de la concursada unase a \n",
      "los autos de su razon para constancia en los mismos, dese \n",
      "traslado a la concursada por plazo de diez dias, para que \n",
      "alegue lo que a su derecho convenga. \n",
      " m odo de impugnacion: recurso de reposicion\n",
      "alegue lo que a su derecho convenga --- Present in Text ---\n"
     ]
    }
   ],
   "source": [
    "f =\"2012_0000026_S5C_20181023464847220181019135535_021_060154700000000159712018060154700111.PDF\"\n",
    "for i,r in fdf.loc[fdf['filename']==f].iterrows():\n",
    "    text = r['text_response'].lower()\n",
    "    text = text.replace('|','')\n",
    "    #text = ' '.join(text.split())\n",
    "    print(text)\n",
    "    for k in N27_key:\n",
    "        for kk in k:\n",
    "            if kk.lower() in text:\n",
    "                print(kk,\"--- Present in Text ---\")\n",
    "        f= fussy_match(k)\n",
    "        if f:\n",
    "            print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Keyword.pickle', 'rb') as handle:\n",
    "    b=pickle.load(handle, encoding='latin1')\n",
    "    \n",
    "kdf=b['keywords']\n",
    "suspkdf=b['susKeyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1431"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in list(kdf.loc[(kdf.fileclass == 'N27')]['keyword'].values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'N9': [['imposibilidad', 'designar', 'domicilio']], \n",
    " 'N16': [['adjudicar ', 'al ejecutante', 'el bien'], ['adjudicar ', 'a la parte ejecutante', 'el bien'], ['adjudicar ', 'a la parte demandante', 'el bien'], ['adjudicar ', 'al ejecutante', 'los bienes'], ['adjudicar ', 'a la parte ejecutante', 'los bienes'], ['adjudicar ', 'a la parte demandante', 'los bienes'], ['consulta a las bases de datos de la aeat', 'telematico', 'bienes'], ['averiguacion', 'telematico', 'bienes']],\n",
    " 'N19': [['se decreta el embargo, por via de mejora'], ['se decreta el embargo, por via de mejora'], ['procedase a instancia', 'por via de mejora, sobre los bienes']],\n",
    " 'N10': [['oigase', 'las partes'], ['concede a la parte', 'para que confirme', 'las cantidades'], ['falta de', 'para que confirme', 'las cantidades'], ['faltan', 'para que confirme', 'las cantidades'], ['habiendose omitido', 'para que confirme', 'las cantidades'], ['requerir', 'para que confirme', 'las cantidades'], ['requeriu', 'para que confirme', 'las cantidades'], ['requierase', 'para que confirme', 'las cantidades'], ['requierasele', 'para que confirme', 'las cantidades'], ['requiero', 'para que confirme', 'las cantidades'], ['se le requiere', 'para que confirme', 'las cantidades'], ['se requiere', 'aporte', 'para que confirme', 'las cantidades'], ['se requiera', 'aporte', 'para que confirme', 'las cantidades'], ['una vez acrediten', 'para que confirme', 'las cantidades'], ['hagase saber a la parte que debe aportar', 'para que confirme', 'las cantidades'], ['plazo subsanar defecto', 'para que confirme', 'las cantidades'], ['requeriu-la', 'a fi de que', 'para que confirme', 'las cantidades'], ['para que en el plazo', 'para que confirme', 'las cantidades'], ['se requiera', 'a fin de que facilita el cif correcto', 'para que confirme', 'las cantidades']],\n",
    " 'N2': [['garcia abascal'], ['acordando que ocupe', 'la posicion ', 'ejecutante'], ['garcia abascal'], ['disposo que ocupi', 'posicio', 'executant'], ['garcia abascal'], ['queda subrogada en la posicion', 'ejecutante']],\n",
    " 'NX-N9': [['si cambiasen su domicilio']], \n",
    " 'N11': [['transferencia a la cuenta bancaria', 'la cantidad'], ['transfierase', 'la cantidad'], ['hagase entrega', 'la cantidad']]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
